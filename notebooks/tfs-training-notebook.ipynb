{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceType": "datasetVersion",
          "sourceId": 12979667,
          "datasetId": 8173756,
          "databundleVersionId": 13642720
        },
        {
          "sourceType": "kernelVersion",
          "sourceId": 259126680
        },
        {
          "sourceType": "kernelVersion",
          "sourceId": 259126739
        },
        {
          "sourceType": "kernelVersion",
          "sourceId": 259126865
        },
        {
          "sourceType": "kernelVersion",
          "sourceId": 259126929
        }
      ],
      "dockerImageVersionId": 31089,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "90178511",
      "cell_type": "markdown",
      "source": "# Transformer training notebook\n\n**Author:** [Giovanni Spadaro](https://giovannispadaro.it/)<br>\n**Project:** [tfs_mt](https://github.com/Giovo17/tfs-mt)<br>\n**Documentation:** [link](https://giovo17.github.io/tfs-mt)",
      "metadata": {}
    },
    {
      "id": "cfdd0b56-6941-493f-a200-7b18bcc2b1fd",
      "cell_type": "code",
      "source": "import os\nif os.path.exists('/kaggle'):\n    PLATFORM = \"KAGGLE\"\nelse:\n    try:\n        import google.colab\n        PLATFORM = \"COLAB\"\n    except:\n        PLATFORM = \"LOCAL\"\nprint(f\"Platform: {PLATFORM}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "0b984966-4b07-4c13-8982-8484111c9803",
      "cell_type": "code",
      "source": "if PLATFORM == \"COLAB\" or PLATFORM == \"KAGGLE\":\n    !pip install -qU wandb --progress-bar off",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "665474d1",
      "cell_type": "code",
      "source": "from pprint import pformat\nfrom functools import partial\nfrom datetime import datetime\n\nfrom torch.optim import AdamW\nfrom torch.nn import CrossEntropyLoss\nimport ignite.distributed as idist\nfrom ignite.engine import Events\nfrom ignite.handlers import PiecewiseLinear\nfrom ignite.metrics import Bleu, Rouge, Loss\nfrom ignite.utils import manual_seed\nfrom omegaconf import OmegaConf\n\nif PLATFORM == \"KAGGLE\":\n    from tfs_mt_architecture.tfs_mt_architecture  import build_model\n    from tfs_mt_data_utils.tfs_mt_data_utils import build_data_utils\n    from tfs_mt_training_utils.tfs_mt_training_utils import *\nelse:\n    from tfs_mt.architecture  import build_model\n    from tfs_mt.data_utils import build_data_utils\n    from tfs_mt.training_utils import *\n    os.chdir(\"..\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "b7f96920-5b85-4fef-a89b-b8eb640b33f6",
      "cell_type": "code",
      "source": "# This block is useful for single GPU training. Ignite will handle devices in multigpu training\nif torch.cuda.is_available():\n    device_ = torch.device(\"cuda:0\")\n    torch.cuda.empty_cache()\n    print(\"Using NVIDIA GPU\")\n    print(f\"Number of available GPUs: {torch.cuda.device_count()}\")\nelse:\n    device_ = torch.device(\"cpu\")\n    print(\"Using CPU\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "d81f4048",
      "cell_type": "code",
      "source": "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # remove tokenizer parallelism warning\n\nif PLATFORM == \"KAGGLE\":\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    wandb_api_key = user_secrets.get_secret(\"WANDB_API_KEY\")    \n    os.environ[\"WANDB_API_KEY\"] = wandb_api_key\n    \n    config_path = \"/kaggle/input/tfs-mt-config/config.yml\"\n    base_path = \"/kaggle/working\"\n    output_dir = \"/kaggle/working/output\"\n    cache_ds_path = \"/kaggle/working/data\"\n    \n    time_limit_sec = 414000  # 11.5 hours\n    \nelif PLATFORM == \"COLAB\":\n    pass\n\nelse:\n    from dotenv import load_dotenv\n    load_dotenv()\n    wandb_api_key = os.getenv(\"WANDB_API_KEY\")\n    os.environ[\"WANDB_API_KEY\"] = wandb_api_key\n    \n    config_path = os.path.join(os.getcwd(), \"tfs_mt/configs/config.yml\")\n    base_path = os.getcwd()\n    output_dir = os.path.join(base_path, \"data/output\")\n    cache_ds_path = os.path.join(base_path, \"data\")\n    time_limit_sec = -1",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "8a80c769-3cba-4e89-ae9d-4068b6633033",
      "cell_type": "code",
      "source": "config = OmegaConf.load(config_path)\n\nconfig.training_hp.distributed_training = True # Enable/disable distributed training\n\nconfig.backend = \"nccl\" if config.training_hp.distributed_training else \"none\"\nconfig.base_path = base_path\nconfig.output_dir = output_dir\nconfig.cache_ds_path = cache_ds_path\nconfig.chosen_model_size = \"nano\"  # nano, small, base\nconfig.time_limit_sec = time_limit_sec\n\nconfig.model_name = f\"{config.model_base_name}_{config.chosen_model_size}_{datetime.now().strftime('%y%m%d-%H%M')}\"",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "76fef81c",
      "cell_type": "code",
      "source": "def run(local_rank, config, distributed=False):\n    \n    if distributed:\n        rank = idist.get_rank()\n        manual_seed(config.seed + rank)\n        output_dir = setup_output_dir(config, rank)\n        config.output_dir = output_dir\n        if rank == 0:\n            save_config(config, output_dir)\n    else:\n        rank = 0\n        manual_seed(config.seed)\n        output_dir = setup_output_dir(config, 0)\n        config.output_dir = output_dir\n        save_config(config, config.output_dir)\n        \n    \n    train_dataloader, test_dataloader, _, _, src_tokenizer, tgt_tokenizer = build_data_utils(config, return_all=True)\n\n    config.num_iters_per_epoch = len(train_dataloader)\n\n    \n    # Initialize model, optimizer, loss function, device\n    device = idist.device() if distributed else device_\n    language_direction = config.dataset.src_lang + \"-\" + config.dataset.tgt_lang\n    init_model = build_model(config, src_tokenizer, tgt_tokenizer)\n    # Get model ready for multigpu training if available and move the model to current device\n    model = idist.auto_model(init_model)\n\n    if distributed: config.training_hp.optimizer_args.learning_rate *= idist.get_world_size()\n    init_optimizer = AdamW(\n        model.parameters(), \n        lr=config.training_hp.optimizer_args.learning_rate, \n        weight_decay=config.training_hp.optimizer_args.weight_decay\n    )\n    # Get model ready for multigpu training if available\n    optimizer = idist.auto_optim(init_optimizer)\n    loss_fn = CrossEntropyLoss(label_smoothing=config.training_hp.loss_label_smoothing).to(device=device)\n\n    le = config.num_iters_per_epoch\n    milestones_values = [\n        (0, 0.0),\n        (le * config.training_hp.num_warmup_epochs, config.training_hp.optimizer_args.learning_rate),\n        (le * config.training_hp.num_epochs, 0.0),\n    ]\n    lr_scheduler = PiecewiseLinear(optimizer, param_name=\"lr\", milestones_values=milestones_values)\n\n    # Setup metrics to attach to evaluator\n    metrics = {\n        \"Bleu\": Bleu(ngram=4, smooth=\"smooth1\", \n                     output_transform=partial(nlp_metric_transform, tgt_tokenizer=tgt_tokenizer)),\n        \"Rouge\": Rouge(variants=[\"L\", 2], multiref=\"best\", \n                       output_transform=partial(nlp_metric_transform, tgt_tokenizer=tgt_tokenizer)),\n        \"Loss\": Loss(loss_fn, output_transform=loss_metric_transform),\n    }\n\n    # Setup trainer and evaluator\n    trainer = setup_trainer(config, model, optimizer, loss_fn, metrics, device, train_dataloader.sampler)\n    evaluator = setup_evaluator(config, model, loss_fn, metrics, device)\n\n    # Setup engines logger with python logging print training configurations\n    logger = setup_logging(config)\n    logger.info(\"Configuration: \\n%s\", pformat(config))\n    trainer.logger = evaluator.logger = logger\n\n    trainer.add_event_handler(Events.ITERATION_COMPLETED, lr_scheduler)\n\n    # Setup ignite handlers\n    to_save_train = {\n        \"model\": model,\n        \"optimizer\": optimizer,\n        \"trainer\": trainer,\n        \"lr_scheduler\": lr_scheduler,\n    }\n    to_save_test = {\"model\": model}\n    ckpt_handler_train, ckpt_handler_test = setup_handlers(trainer, evaluator, config, to_save_train, to_save_test)\n\n    # Experiment tracking\n    if rank == 0:\n        print(config.model_name)\n        exp_logger = setup_exp_logging(config, trainer, optimizer, evaluator)\n\n    # Print metrics to the stderr with \"add_event_handler\" method for training stats\n    trainer.add_event_handler(\n        Events.ITERATION_COMPLETED(every=config.log_every_iters),\n        log_metrics,\n        tag=\"train\",\n    )\n\n    # Run evaluator at every training epoch end using \"on\" decorator method and print metrics to the stderr\n    # More on ignite Events: https://docs.pytorch.org/ignite/generated/ignite.engine.events.Events.html\n    @trainer.on(Events.EPOCH_COMPLETED(every=1))\n    def _():\n        evaluator.run(test_dataloader)\n        log_metrics(evaluator, \"test\")\n\n    # Run evaluator when trainer starts to make sure it works \n    @trainer.on(Events.STARTED)\n    def _():\n        evaluator.run(test_dataloader)\n\n    trainer.run(\n        train_dataloader,\n        max_epochs=config.training_hp.num_epochs,\n    )\n\n    if rank == 0:\n        exp_logger.close()\n    \n    logger.info(f\"Last training checkpoint name - {ckpt_handler_train.last_checkpoint}\")\n    logger.info(f\"Last testing checkpoint name - {ckpt_handler_test.last_checkpoint}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "1cc4c186-2ca7-4a3b-8c50-c2efd5887db6",
      "cell_type": "code",
      "source": "if config.training_hp.distributed_training:\n    with idist.Parallel(config.backend) as p:\n        p.run(run, config, distributed=True)\nelse:\n    run(0, config)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}
