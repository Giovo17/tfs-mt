{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"tfs-mt Transformer from scratch for Machine Translation          \ud83c\udfe0 Homepage          \u2022              \u25b6\ufe0f Getting started          \u2022              \ud83e\udd17 Hugging Face          \u2022              \ud83c\udfac Demo      <p>This project implements the Transformer architecture from scratch considering Machine Translation as the usecase. It's mainly intended as an educational resource and a functional implementation of the architecture and the training/inference logic.</p>"},{"location":"#getting-started","title":"Getting started","text":""},{"location":"#from-pip","title":"From pip","text":"<pre><code>pip install tfs-mt\n</code></pre>"},{"location":"#from-source","title":"From source","text":""},{"location":"#prerequisites","title":"Prerequisites","text":"<ul> <li><code>uv</code> [install]</li> </ul>"},{"location":"#steps","title":"Steps","text":"<pre><code>git clone https://github.com/Giovo17/tfs-mt.git\ncd tfs-mt\n\nuv sync\n\ncp .env.example .env\n# Edit .env file with your configuration\n</code></pre>"},{"location":"#usage","title":"Usage","text":""},{"location":"#training","title":"Training","text":"<p>To start training the model with the default configuration:</p> <pre><code>uv run src/train.py\n</code></pre>"},{"location":"#inference","title":"Inference","text":"<p>To run inference using the trained model from the HuggingFace repo:</p> <pre><code>uv run src/inference.py\n</code></pre>"},{"location":"#configuration","title":"Configuration","text":"<p>The whole project parameters can be configured in <code>src/tfs_mt/configs/config.yml</code>. Key configurations include:</p> <ul> <li>Model Architecture: Config, dropout, GloVe embedding init, ...</li> <li>Training: Optimizer, Learning rate scheduler, number of epochs, ...</li> <li>Data: Dataset, Dataloader, Tokenizer, ...</li> </ul>"},{"location":"#architecture","title":"Architecture","text":"<p>For a detailed explanation of the architecture and design choices, please refer to the Architecture Documentation.</p>"},{"location":"#model-sizes","title":"Model sizes","text":"<p>The project supports various model configurations to suit different computational resources:</p> Parameter Nano Small Base Original Encoder Layers 4 6 8 6 Decoder Layers 4 6 8 6 d_model 50 100 300 512 Num Heads 4 6 8 8 d_ff 200 400 800 2048 Norm Type PostNorm PostNorm PostNorm PostNorm Dropout 0.1 0.1 0.1 0.1 GloVe Dim 50d 100d 300d -"},{"location":"#citation","title":"Citation","text":"<p>If you use <code>tfs-mt</code> in your research or project, please cite:</p> <pre><code>@software{Spadaro_tfs-mt,\nauthor = {Spadaro, Giovanni},\nlicense = {MIT},\ntitle = {{tfs-mt}},\nurl = {https://github.com/Giovo17/tfs-mt}\n}\n</code></pre>"},{"location":"architecture_explain/","title":"Architecture","text":""},{"location":"architecture_explain/#the-transformer-architecture","title":"The Transformer architecture","text":"<p>Disclaimer</p> <p>This Transformer implementation misses a lot of architectural improvements that have been developed since the first paper release in 2017. It's main purpose is to show off the architecture design and training methods as a learning project.</p>"},{"location":"architecture_explain/#introduction","title":"Introduction","text":"<p>The Transformer architecture was first introduced in the paper \"Attention Is All You Need\" by Google[^vaswani2023attentionneed] as an alternative to recurrent or convolution-based networks for sequence processing. Since then it has been a disruptive architecture achieving important results in Natural Language Processing and in many other field, such as Computer Vision, Audio processing, Generative AI and Multimodal learning.</p> <p></p> <p>Caption. [^build-llms-from-scratch-book]</p>"},{"location":"modules/architecture/","title":"Architecture","text":""},{"location":"modules/architecture/#src.tfs_mt.architecture.DecoderBlock","title":"<code>DecoderBlock</code>","text":"<p>               Bases: <code>Module</code></p> <p>Transformer Decoder block.</p> <p>Using prenorm approach as in EncoderBlock.</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>Model dimension.</p> required <code>num_heads</code> <code>int</code> <p>Number of attention heads.</p> required <code>d_ff</code> <code>int</code> <p>Size of middle feedforward layer.</p> required <code>dropout_prob</code> <code>float</code> <p>Dropout probability. Defaults to 0.1.</p> <code>0.1</code> <code>norm_type</code> <code>str</code> <p>Layer normalization strategy. Defaults to \"postnorm\".</p> <code>'postnorm'</code> Source code in <code>src/tfs_mt/architecture.py</code> <pre><code>class DecoderBlock(nn.Module):\n    \"\"\"Transformer Decoder block.\n\n    Using prenorm approach as in EncoderBlock.\n\n    Args:\n        d_model (int): Model dimension.\n        num_heads (int): Number of attention heads.\n        d_ff (int): Size of middle feedforward layer.\n        dropout_prob (float, optional): Dropout probability. Defaults to 0.1.\n        norm_type (str, optional): Layer normalization strategy. Defaults to \"postnorm\".\n    \"\"\"\n\n    def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout_prob: float = 0.1, norm_type: str = \"postnorm\"):\n        super().__init__()\n\n        self.self_attention = MultiHeadAttention(d_model, num_heads, dropout_prob)\n        self.cross_attention = MultiHeadAttention(d_model, num_heads, dropout_prob)\n        self.feedforward = FeedForward(d_model, d_ff, dropout_prob)\n        self.layer_norm1 = LayerNorm(d_model)\n        self.layer_norm2 = LayerNorm(d_model)\n        self.layer_norm3 = LayerNorm(d_model)\n        # Layer norm at the end of the block for prenorm configuration\n        if norm_type == \"prenorm\":\n            self.layer_norm4 = LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout_prob)\n        self.norm_type = norm_type\n\n    def forward(\n        self,\n        x: Float[torch.Tensor, \"B S D\"],\n        encoder_representation: Float[torch.Tensor, \"B S D\"],\n        tgt_mask: Bool[torch.Tensor, \"B 1 S S\"],\n        src_mask: Bool[torch.Tensor, \"B 1 S S\"],\n    ) -&gt; Float[torch.Tensor, \"B S D\"]:\n        if self.norm_type == \"postnorm\":\n            return self.postnorm_forward(x, encoder_representation, tgt_mask, src_mask)\n        return self.prenorm_forward(x, encoder_representation, tgt_mask, src_mask)\n\n    def postnorm_forward(\n        self,\n        x: Float[torch.Tensor, \"B S D\"],\n        encoder_representation: Float[torch.Tensor, \"B S D\"],\n        tgt_mask: Bool[torch.Tensor, \"B 1 S S\"],\n        src_mask: Bool[torch.Tensor, \"B 1 S S\"],\n    ) -&gt; torch.Tensor:\n        \"\"\"Original Postnorm forward function. Accoding to the following paper it outperforms Prenorm in zero-shot machine translation, https://arxiv.org/abs/2305.09312.\"\"\"\n        t1 = self.self_attention(x_query=x, x_key=x, x_value=x, attention_mask=tgt_mask)\n        # We apply dropout to the output of each sub-layer, before it is added to the sub-layer input and normalized (Attention is all you need page 8)\n        t2 = self.dropout(t1)\n        t3 = t2 + x\n        t4 = self.layer_norm1(t3)\n\n        t5 = self.cross_attention(\n            x_query=t4, x_key=encoder_representation, x_value=encoder_representation, attention_mask=src_mask\n        )\n\n        t6 = self.dropout(t5)\n        t7 = t6 + t4\n        t8 = self.layer_norm2(t7)\n\n        t9 = self.feedforward(t8)\n        t10 = self.dropout(t9)\n        t11 = t10 + t8\n        h = self.layer_norm3(t11)\n\n        return h\n\n    def prenorm_forward(\n        self,\n        x: Float[torch.Tensor, \"B S D\"],\n        encoder_representation: Float[torch.Tensor, \"B S D\"],\n        tgt_mask: Bool[torch.Tensor, \"B 1 S S\"],\n        src_mask: Bool[torch.Tensor, \"B 1 S S\"],\n    ) -&gt; torch.Tensor:\n        \"\"\"Prenorm forward function. More on https://arxiv.org/abs/2002.04745.\"\"\"\n        t1 = self.layer_norm1(x)\n        t2 = self.self_attention(x_query=t1, x_key=t1, x_value=t1, attention_mask=tgt_mask)\n        t3 = self.dropout(t2)\n        t4 = t3 + x\n\n        t5 = self.layer_norm2(t4)\n        t6 = self.cross_attention(\n            x_query=t5, x_key=encoder_representation, x_value=encoder_representation, attention_mask=src_mask\n        )\n        t7 = self.dropout(t6)\n        t8 = t7 + t4\n\n        t9 = self.layer_norm3(t8)\n        t10 = self.feedforward(t9)\n        t11 = self.dropout(t10)\n        t12 = t11 + t8\n\n        h = self.layer_norm4(t12)\n\n        return h\n</code></pre>"},{"location":"modules/architecture/#src.tfs_mt.architecture.DecoderBlock.postnorm_forward","title":"<code>postnorm_forward(x, encoder_representation, tgt_mask, src_mask)</code>","text":"<p>Original Postnorm forward function. Accoding to the following paper it outperforms Prenorm in zero-shot machine translation, https://arxiv.org/abs/2305.09312.</p> Source code in <code>src/tfs_mt/architecture.py</code> <pre><code>def postnorm_forward(\n    self,\n    x: Float[torch.Tensor, \"B S D\"],\n    encoder_representation: Float[torch.Tensor, \"B S D\"],\n    tgt_mask: Bool[torch.Tensor, \"B 1 S S\"],\n    src_mask: Bool[torch.Tensor, \"B 1 S S\"],\n) -&gt; torch.Tensor:\n    \"\"\"Original Postnorm forward function. Accoding to the following paper it outperforms Prenorm in zero-shot machine translation, https://arxiv.org/abs/2305.09312.\"\"\"\n    t1 = self.self_attention(x_query=x, x_key=x, x_value=x, attention_mask=tgt_mask)\n    # We apply dropout to the output of each sub-layer, before it is added to the sub-layer input and normalized (Attention is all you need page 8)\n    t2 = self.dropout(t1)\n    t3 = t2 + x\n    t4 = self.layer_norm1(t3)\n\n    t5 = self.cross_attention(\n        x_query=t4, x_key=encoder_representation, x_value=encoder_representation, attention_mask=src_mask\n    )\n\n    t6 = self.dropout(t5)\n    t7 = t6 + t4\n    t8 = self.layer_norm2(t7)\n\n    t9 = self.feedforward(t8)\n    t10 = self.dropout(t9)\n    t11 = t10 + t8\n    h = self.layer_norm3(t11)\n\n    return h\n</code></pre>"},{"location":"modules/architecture/#src.tfs_mt.architecture.DecoderBlock.prenorm_forward","title":"<code>prenorm_forward(x, encoder_representation, tgt_mask, src_mask)</code>","text":"<p>Prenorm forward function. More on https://arxiv.org/abs/2002.04745.</p> Source code in <code>src/tfs_mt/architecture.py</code> <pre><code>def prenorm_forward(\n    self,\n    x: Float[torch.Tensor, \"B S D\"],\n    encoder_representation: Float[torch.Tensor, \"B S D\"],\n    tgt_mask: Bool[torch.Tensor, \"B 1 S S\"],\n    src_mask: Bool[torch.Tensor, \"B 1 S S\"],\n) -&gt; torch.Tensor:\n    \"\"\"Prenorm forward function. More on https://arxiv.org/abs/2002.04745.\"\"\"\n    t1 = self.layer_norm1(x)\n    t2 = self.self_attention(x_query=t1, x_key=t1, x_value=t1, attention_mask=tgt_mask)\n    t3 = self.dropout(t2)\n    t4 = t3 + x\n\n    t5 = self.layer_norm2(t4)\n    t6 = self.cross_attention(\n        x_query=t5, x_key=encoder_representation, x_value=encoder_representation, attention_mask=src_mask\n    )\n    t7 = self.dropout(t6)\n    t8 = t7 + t4\n\n    t9 = self.layer_norm3(t8)\n    t10 = self.feedforward(t9)\n    t11 = self.dropout(t10)\n    t12 = t11 + t8\n\n    h = self.layer_norm4(t12)\n\n    return h\n</code></pre>"},{"location":"modules/architecture/#src.tfs_mt.architecture.EncoderBlock","title":"<code>EncoderBlock</code>","text":"<p>               Bases: <code>Module</code></p> <p>Transformer Encoder block.</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>Model dimension.</p> required <code>num_heads</code> <code>int</code> <p>Number of attention heads.</p> required <code>d_ff</code> <code>int</code> <p>Size of middle feedforward layer.</p> required <code>dropout_prob</code> <code>float</code> <p>Dropout probability. Defaults to 0.1.</p> <code>0.1</code> <code>norm_type</code> <code>str</code> <p>Layer normalization strategy. Defaults to \"postnorm\".</p> <code>'postnorm'</code> Source code in <code>src/tfs_mt/architecture.py</code> <pre><code>class EncoderBlock(nn.Module):\n    \"\"\"Transformer Encoder block.\n\n    Args:\n        d_model (int): Model dimension.\n        num_heads (int): Number of attention heads.\n        d_ff (int): Size of middle feedforward layer.\n        dropout_prob (float, optional): Dropout probability. Defaults to 0.1.\n        norm_type (str, optional): Layer normalization strategy. Defaults to \"postnorm\".\n    \"\"\"\n\n    def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout_prob: float = 0.1, norm_type: str = \"postnorm\"):\n        super().__init__()\n\n        self.self_attention = MultiHeadAttention(d_model, num_heads, dropout_prob)\n        self.feedforward = FeedForward(d_model, d_ff, dropout_prob)\n        self.layer_norm1 = LayerNorm(d_model)\n        self.layer_norm2 = LayerNorm(d_model)\n        # Layer norm at the end of the block for prenorm configuration\n        if norm_type == \"prenorm\":\n            self.layer_norm3 = LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout_prob)\n        self.norm_type = norm_type\n\n    def forward(\n        self, x: Float[torch.Tensor, \"B S D\"], attention_mask: Bool[torch.Tensor, \"B 1 S S\"]\n    ) -&gt; Float[torch.Tensor, \"B S D\"]:\n        if self.norm_type == \"postnorm\":\n            return self.postnorm_forward(x, attention_mask)\n        return self.prenorm_forward(x, attention_mask)\n\n    def postnorm_forward(\n        self, x: Float[torch.Tensor, \"B S D\"], attention_mask: Bool[torch.Tensor, \"B 1 S S\"]\n    ) -&gt; Float[torch.Tensor, \"B S D\"]:\n        \"\"\"Original Postnorm forward function. Accoding to the following paper it outperforms Prenorm in zero-shot machine translation, https://arxiv.org/abs/2305.09312.\"\"\"\n        t1 = self.self_attention(x_query=x, x_key=x, x_value=x, attention_mask=attention_mask)\n        # We apply dropout to the output of each sub-layer, before it is added to the sub-layer input and normalized (Attention is all you need page 8)\n        t2 = self.dropout(t1)\n        t3 = t2 + x\n        t4 = self.layer_norm1(t3)\n\n        t5 = self.feedforward(t4)\n        t6 = self.dropout(t5)\n        t7 = t6 + t4\n        h = self.layer_norm2(t7)\n\n        return h\n\n    def prenorm_forward(\n        self, x: Float[torch.Tensor, \"B S D\"], attention_mask: Bool[torch.Tensor, \"B 1 S S\"]\n    ) -&gt; Float[torch.Tensor, \"B S D\"]:\n        \"\"\"Prenorm forward function. More on https://arxiv.org/abs/2002.04745.\"\"\"\n        t1 = self.layer_norm1(x)\n        t2 = self.self_attention(x_query=t1, x_key=t1, x_value=t1, attention_mask=attention_mask)\n        t3 = self.dropout(t2)\n        t4 = t3 + x\n\n        t5 = self.layer_norm2(t4)\n        t6 = self.feedforward(t5)\n        t7 = self.dropout(t6)\n        t8 = t7 + t4\n\n        h = self.layer_norm3(t8)\n\n        return h\n</code></pre>"},{"location":"modules/architecture/#src.tfs_mt.architecture.EncoderBlock.postnorm_forward","title":"<code>postnorm_forward(x, attention_mask)</code>","text":"<p>Original Postnorm forward function. Accoding to the following paper it outperforms Prenorm in zero-shot machine translation, https://arxiv.org/abs/2305.09312.</p> Source code in <code>src/tfs_mt/architecture.py</code> <pre><code>def postnorm_forward(\n    self, x: Float[torch.Tensor, \"B S D\"], attention_mask: Bool[torch.Tensor, \"B 1 S S\"]\n) -&gt; Float[torch.Tensor, \"B S D\"]:\n    \"\"\"Original Postnorm forward function. Accoding to the following paper it outperforms Prenorm in zero-shot machine translation, https://arxiv.org/abs/2305.09312.\"\"\"\n    t1 = self.self_attention(x_query=x, x_key=x, x_value=x, attention_mask=attention_mask)\n    # We apply dropout to the output of each sub-layer, before it is added to the sub-layer input and normalized (Attention is all you need page 8)\n    t2 = self.dropout(t1)\n    t3 = t2 + x\n    t4 = self.layer_norm1(t3)\n\n    t5 = self.feedforward(t4)\n    t6 = self.dropout(t5)\n    t7 = t6 + t4\n    h = self.layer_norm2(t7)\n\n    return h\n</code></pre>"},{"location":"modules/architecture/#src.tfs_mt.architecture.EncoderBlock.prenorm_forward","title":"<code>prenorm_forward(x, attention_mask)</code>","text":"<p>Prenorm forward function. More on https://arxiv.org/abs/2002.04745.</p> Source code in <code>src/tfs_mt/architecture.py</code> <pre><code>def prenorm_forward(\n    self, x: Float[torch.Tensor, \"B S D\"], attention_mask: Bool[torch.Tensor, \"B 1 S S\"]\n) -&gt; Float[torch.Tensor, \"B S D\"]:\n    \"\"\"Prenorm forward function. More on https://arxiv.org/abs/2002.04745.\"\"\"\n    t1 = self.layer_norm1(x)\n    t2 = self.self_attention(x_query=t1, x_key=t1, x_value=t1, attention_mask=attention_mask)\n    t3 = self.dropout(t2)\n    t4 = t3 + x\n\n    t5 = self.layer_norm2(t4)\n    t6 = self.feedforward(t5)\n    t7 = self.dropout(t6)\n    t8 = t7 + t4\n\n    h = self.layer_norm3(t8)\n\n    return h\n</code></pre>"},{"location":"modules/architecture/#src.tfs_mt.architecture.MultiHeadAttention","title":"<code>MultiHeadAttention</code>","text":"<p>               Bases: <code>Module</code></p> <p>MultiHead Attention for Transformer encoder and decoder. It handles both self and cross attention operations.</p> <p>Following the implementation described in Speech and Language Processing by Daniel Jurafsky [link].</p> <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>Model dimension.</p> required <code>num_heads</code> <code>int</code> <p>Number of attention heads.</p> required <code>dropout_prob</code> <code>float</code> <p>Dropout probability. Defaults to 0.1.</p> <code>0.1</code> Source code in <code>src/tfs_mt/architecture.py</code> <pre><code>class MultiHeadAttention(nn.Module):\n    \"\"\"MultiHead Attention for Transformer encoder and decoder. It handles both self and cross attention operations.\n\n    Following the implementation described in *Speech and Language Processing* by *Daniel Jurafsky* [[link](https://web.stanford.edu/~jurafsky/slp3/)].\n\n    Args:\n        d_model (int): Model dimension.\n        num_heads (int): Number of attention heads.\n        dropout_prob (float, optional): Dropout probability. Defaults to 0.1.\n    \"\"\"\n\n    def __init__(self, d_model: int, num_heads: int, dropout_prob: float = 0.1):\n        super().__init__()\n\n        self.d_model = d_model\n        self.num_heads = num_heads\n\n        # NOTE Usually transformer models have d_model divisible by num_heads.\n        # This guarantees that the attention heads' outputs are d_model shaped vectors when stacked together (considering each embedding vector)\n        # In this implementation it has been preferred to remove this constraint. When exploiting the support of GloVe pretrained embeddings,\n        # d_model is fixed to GloVe embeddings sizes, namely 25, 50, 100, 200, 300, so in this scenario num_heads would be limited to predefined set of values due to int quantization.\n        # Considering the intermidiate output dimensions there will be no problems since the 3 initial projections matrices' shapes have been adjusted in order to map\n        # from d_model to num_heads * self.d_head (see below why the projection matrices are not splitted into head-specific matrices).\n        # Considering the final output dimensions the W_O matrix will project num_heads*d_head dimensional vectors into d_model vectors, so the whole operation\n        # continues to be mathematically consistent ensuring input and output dimension of this module are the same.\n        # eg. d_model = 50, num_heads = 4, d_head = int(d_model/num_heads) = 12\n        # q = x * W_Q   q shape is 48 (same goes to k and v), W_Q shape is 50x48   (in this example q is the concatention of the q vectors )\n        # output = attention_output * W_O   output shape is 50, W_O shape is 48x50\n        # if d_model % num_heads != 0:\n        #    raise DimError(d_model, num_heads)\n\n        self.d_head = int(d_model / num_heads)  # Query, key and value embeddings dimension. d_k = d_v = d_head\n\n        # Learnable projection matrices. Bias term is omitted since they are used as projections matrices.\n        # Every head should have its projection matrix, but rather considering a set of QKV matrices for each head,\n        # here 3 bigger matrices are considered. The following example involves the query projection matrix W_Q but the reasoning applies to all of them.\n        # Considering D = d_model, d_k = d_v = d_head and A = num_heads.\n        # W_Q is a DxD matrix and each W_Q_i (query projection matrix for i-th head) should be a DxD_k matrix.\n        # W_Q can be reshaped as a DxAxD_k matrix since A*d_k = D due to initial assertion. (in practice the output projection will be reshaped as mentioned)\n        # This way we can properly take advantage of GPU parallelization thanks to torch broadcasting,\n        # instead of executing one projection operation at a time for each head in a loop.\n        self.W_Q = nn.Linear(d_model, num_heads * self.d_head, bias=False)\n        self.W_K = nn.Linear(d_model, num_heads * self.d_head, bias=False)\n        self.W_V = nn.Linear(d_model, num_heads * self.d_head, bias=False)\n        self.W_O = nn.Linear(num_heads * self.d_head, d_model, bias=False)  # Output projection\n\n        self.dropout = nn.Dropout(dropout_prob)\n\n        self.scaling_factor = math.sqrt(self.d_head)  # To avoid computing it every time attention method is called\n\n    def forward(\n        self,\n        x_query: Float[torch.Tensor, \"B S D\"],\n        x_key: Float[torch.Tensor, \"B S D\"],\n        x_value: Float[torch.Tensor, \"B S D\"],\n        attention_mask: Bool[torch.Tensor, \"B 1 S S\"] | None = None,\n    ) -&gt; Float[torch.Tensor, \"B S D\"]:\n        \"\"\"MultiHead attention.\n\n        Args:\n            x_query (Float[torch.Tensor, \"B S D\"]): Matrix of input query embeddings. B is the batch size, S is the sequence length and D is d_model.\n            x_key (Float[torch.Tensor, \"B S D\"]): Matrix of input key embeddings.\n            x_value (Float[torch.Tensor, \"B S D\"]): Matrix of input value embeddings.\n            attention_mask (Bool[torch.Tensor, \"B 1 S S\"] | None, optional): Attention mask to avoid computing attention to padding tokens. It's also used to apply causal masking in decoder self attention. Defaults to None.\n\n        Returns:\n            Float[torch.Tensor, \"B S D\"]: Processed output tensor.\n        \"\"\"\n        batch_size = x_query.shape[0]\n\n        # W_Q(x)          [B, S, D]\n        # After reshape   [B, S, A, d_k]\n        # After transpose [B, A, S, d_k] where A is num_heads, d_k is the head dimension\n        query_matrices = self.W_Q(x_query).reshape(batch_size, -1, self.num_heads, self.d_head).transpose(1, 2)\n        key_matrices = self.W_K(x_key).reshape(batch_size, -1, self.num_heads, self.d_head).transpose(1, 2)\n        value_matrices = self.W_V(x_value).reshape(batch_size, -1, self.num_heads, self.d_head).transpose(1, 2)\n\n        # Concatenated heads outputs\n        attn_output = self.attention(query_matrices, key_matrices, value_matrices, attention_mask=attention_mask)\n\n        # Reshape to [B, S, A*d_k]\n        attn_output_reshaped = attn_output.transpose(1, 2).reshape(batch_size, -1, self.num_heads * self.d_head)\n\n        # Attention scores, shape [B, S, D]. Combine heads outputs into a single D-dimensional output.\n        return self.W_O(attn_output_reshaped)\n\n    def attention(\n        self,\n        query: Float[torch.Tensor, \"B A S d_k\"],\n        key: Float[torch.Tensor, \"B A S d_k\"],\n        value: Float[torch.Tensor, \"B A S d_k\"],\n        attention_mask: Bool[torch.Tensor, \"B 1 S S\"] | None = None,\n    ) -&gt; Float[torch.Tensor, \"B A S d_k\"]:\n        \"\"\"Implements attention as follows:\n\n        $$\n        Attention(Q,K,V) = softmax ( \\\\frac{QK^T}{\\\\sqrt{d_{head}}} ) * V\n        $$\n\n        Args:\n            query (Float[torch.Tensor, \"B, A, S, d_k\"]): Matrix of input query embeddings. Where B is the batch size, A is the number of heads, S is the sequence length and d_k is the head dimension.\n            key (Float[torch.Tensor, \"B, A, S, d_k\"]): Matrix of input key embeddings.\n            value (Float[torch.Tensor, \"B, A, S, d_k\"]): Matrix of input value embeddings.\n            attention_mask (Bool[torch.Tensor, \"B 1 S S\"] | None, optional): Attention mask to avoid computing attention to padding tokens. It's also used to apply causal masking in decoder self attention. Defaults to None.\n\n        Returns:\n            Float[torch.Tensor, \"B, A, S, d_k\"]: Attention matrix.\n        \"\"\"\n        QKt = torch.matmul(query, key.transpose(-1, -2)) / self.scaling_factor\n\n        if attention_mask is not None:\n            # NOTE Adding this control to correctly process masking considering that target input sequence will be shrinked by one token\n            # This is especially needed when computing cross attention in decoder blocks due to the usage of src_mask which cannot be shrinked accordingly a priori\n            if attention_mask.shape[-1] &gt; QKt.shape[-1] or attention_mask.shape[-2] &gt; QKt.shape[-2]:\n                attention_mask = attention_mask[:, :, : QKt.shape[-2], : QKt.shape[-1]]\n            QKt.masked_fill_(attention_mask == False, float(\"-inf\"))\n\n        # Applying the softmax on last dim makes results in a QKt matrix with normalized rows\n        QKt_norm = torch.softmax(QKt, dim=-1)\n\n        # Store attention weights for visualization purposes\n        self.attn_weights = QKt_norm.detach()\n\n        QKt_norm = self.dropout(QKt_norm)\n\n        # Fix nan propagation due to softmax processing of masked matrix containing entire rows full of -inf\n        QKt_norm = QKt_norm.masked_fill(torch.isnan(QKt_norm), 0.0)\n\n        return torch.matmul(QKt_norm, value)\n</code></pre>"},{"location":"modules/architecture/#src.tfs_mt.architecture.MultiHeadAttention.attention","title":"<code>attention(query, key, value, attention_mask=None)</code>","text":"<p>Implements attention as follows:</p> \\[ Attention(Q,K,V) = softmax ( \\frac{QK^T}{\\sqrt{d_{head}}} ) * V \\] <p>Parameters:</p> Name Type Description Default <code>query</code> <code>Float[Tensor, (B, A, S, d_k)]</code> <p>Matrix of input query embeddings. Where B is the batch size, A is the number of heads, S is the sequence length and d_k is the head dimension.</p> required <code>key</code> <code>Float[Tensor, (B, A, S, d_k)]</code> <p>Matrix of input key embeddings.</p> required <code>value</code> <code>Float[Tensor, (B, A, S, d_k)]</code> <p>Matrix of input value embeddings.</p> required <code>attention_mask</code> <code>Bool[Tensor, 'B 1 S S'] | None</code> <p>Attention mask to avoid computing attention to padding tokens. It's also used to apply causal masking in decoder self attention. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Float[Tensor, 'B A S d_k']</code> <p>Float[torch.Tensor, \"B, A, S, d_k\"]: Attention matrix.</p> Source code in <code>src/tfs_mt/architecture.py</code> <pre><code>def attention(\n    self,\n    query: Float[torch.Tensor, \"B A S d_k\"],\n    key: Float[torch.Tensor, \"B A S d_k\"],\n    value: Float[torch.Tensor, \"B A S d_k\"],\n    attention_mask: Bool[torch.Tensor, \"B 1 S S\"] | None = None,\n) -&gt; Float[torch.Tensor, \"B A S d_k\"]:\n    \"\"\"Implements attention as follows:\n\n    $$\n    Attention(Q,K,V) = softmax ( \\\\frac{QK^T}{\\\\sqrt{d_{head}}} ) * V\n    $$\n\n    Args:\n        query (Float[torch.Tensor, \"B, A, S, d_k\"]): Matrix of input query embeddings. Where B is the batch size, A is the number of heads, S is the sequence length and d_k is the head dimension.\n        key (Float[torch.Tensor, \"B, A, S, d_k\"]): Matrix of input key embeddings.\n        value (Float[torch.Tensor, \"B, A, S, d_k\"]): Matrix of input value embeddings.\n        attention_mask (Bool[torch.Tensor, \"B 1 S S\"] | None, optional): Attention mask to avoid computing attention to padding tokens. It's also used to apply causal masking in decoder self attention. Defaults to None.\n\n    Returns:\n        Float[torch.Tensor, \"B, A, S, d_k\"]: Attention matrix.\n    \"\"\"\n    QKt = torch.matmul(query, key.transpose(-1, -2)) / self.scaling_factor\n\n    if attention_mask is not None:\n        # NOTE Adding this control to correctly process masking considering that target input sequence will be shrinked by one token\n        # This is especially needed when computing cross attention in decoder blocks due to the usage of src_mask which cannot be shrinked accordingly a priori\n        if attention_mask.shape[-1] &gt; QKt.shape[-1] or attention_mask.shape[-2] &gt; QKt.shape[-2]:\n            attention_mask = attention_mask[:, :, : QKt.shape[-2], : QKt.shape[-1]]\n        QKt.masked_fill_(attention_mask == False, float(\"-inf\"))\n\n    # Applying the softmax on last dim makes results in a QKt matrix with normalized rows\n    QKt_norm = torch.softmax(QKt, dim=-1)\n\n    # Store attention weights for visualization purposes\n    self.attn_weights = QKt_norm.detach()\n\n    QKt_norm = self.dropout(QKt_norm)\n\n    # Fix nan propagation due to softmax processing of masked matrix containing entire rows full of -inf\n    QKt_norm = QKt_norm.masked_fill(torch.isnan(QKt_norm), 0.0)\n\n    return torch.matmul(QKt_norm, value)\n</code></pre>"},{"location":"modules/architecture/#src.tfs_mt.architecture.MultiHeadAttention.forward","title":"<code>forward(x_query, x_key, x_value, attention_mask=None)</code>","text":"<p>MultiHead attention.</p> <p>Parameters:</p> Name Type Description Default <code>x_query</code> <code>Float[Tensor, 'B S D']</code> <p>Matrix of input query embeddings. B is the batch size, S is the sequence length and D is d_model.</p> required <code>x_key</code> <code>Float[Tensor, 'B S D']</code> <p>Matrix of input key embeddings.</p> required <code>x_value</code> <code>Float[Tensor, 'B S D']</code> <p>Matrix of input value embeddings.</p> required <code>attention_mask</code> <code>Bool[Tensor, 'B 1 S S'] | None</code> <p>Attention mask to avoid computing attention to padding tokens. It's also used to apply causal masking in decoder self attention. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Float[Tensor, 'B S D']</code> <p>Float[torch.Tensor, \"B S D\"]: Processed output tensor.</p> Source code in <code>src/tfs_mt/architecture.py</code> <pre><code>def forward(\n    self,\n    x_query: Float[torch.Tensor, \"B S D\"],\n    x_key: Float[torch.Tensor, \"B S D\"],\n    x_value: Float[torch.Tensor, \"B S D\"],\n    attention_mask: Bool[torch.Tensor, \"B 1 S S\"] | None = None,\n) -&gt; Float[torch.Tensor, \"B S D\"]:\n    \"\"\"MultiHead attention.\n\n    Args:\n        x_query (Float[torch.Tensor, \"B S D\"]): Matrix of input query embeddings. B is the batch size, S is the sequence length and D is d_model.\n        x_key (Float[torch.Tensor, \"B S D\"]): Matrix of input key embeddings.\n        x_value (Float[torch.Tensor, \"B S D\"]): Matrix of input value embeddings.\n        attention_mask (Bool[torch.Tensor, \"B 1 S S\"] | None, optional): Attention mask to avoid computing attention to padding tokens. It's also used to apply causal masking in decoder self attention. Defaults to None.\n\n    Returns:\n        Float[torch.Tensor, \"B S D\"]: Processed output tensor.\n    \"\"\"\n    batch_size = x_query.shape[0]\n\n    # W_Q(x)          [B, S, D]\n    # After reshape   [B, S, A, d_k]\n    # After transpose [B, A, S, d_k] where A is num_heads, d_k is the head dimension\n    query_matrices = self.W_Q(x_query).reshape(batch_size, -1, self.num_heads, self.d_head).transpose(1, 2)\n    key_matrices = self.W_K(x_key).reshape(batch_size, -1, self.num_heads, self.d_head).transpose(1, 2)\n    value_matrices = self.W_V(x_value).reshape(batch_size, -1, self.num_heads, self.d_head).transpose(1, 2)\n\n    # Concatenated heads outputs\n    attn_output = self.attention(query_matrices, key_matrices, value_matrices, attention_mask=attention_mask)\n\n    # Reshape to [B, S, A*d_k]\n    attn_output_reshaped = attn_output.transpose(1, 2).reshape(batch_size, -1, self.num_heads * self.d_head)\n\n    # Attention scores, shape [B, S, D]. Combine heads outputs into a single D-dimensional output.\n    return self.W_O(attn_output_reshaped)\n</code></pre>"},{"location":"modules/architecture/#src.tfs_mt.architecture.Transformer","title":"<code>Transformer</code>","text":"<p>               Bases: <code>Module</code></p> <p>Transformer model.</p> <p>Using Language Model head to map decoder output representation to tokens in vocabulary.</p> <p>Parameters:</p> Name Type Description Default <code>src_vocab_size</code> <code>int</code> <p>Size of source language vocabulary.</p> required <code>tgt_vocab_size</code> <code>int</code> <p>Size of target language vocabulary.</p> required <code>num_encoder_blocks</code> <code>int</code> <p>Number of encoder blocks. Defaults to 6.</p> <code>6</code> <code>num_decoder_blocks</code> <code>int</code> <p>Number of decoder blocks. Defaults to 6.</p> <code>6</code> <code>d_model</code> <code>int</code> <p>Model dimension. Defaults to 512.</p> <code>512</code> <code>num_heads</code> <code>int</code> <p>Number of heads in MultiHead Attention. Defaults to 8.</p> <code>8</code> <code>d_ff</code> <code>int</code> <p>Size of middle feedforward layer. Defaults to 2048.</p> <code>2048</code> <code>norm_type</code> <code>str</code> <p>Layer normalization strategy. Defaults to \"postnorm\".</p> <code>'postnorm'</code> <code>dropout_prob</code> <code>float</code> <p>Dropout probability. Defaults to 0.1.</p> <code>0.1</code> <code>max_seq_len</code> <code>int</code> <p>Max sequence length. Defaults to 128.</p> <code>128</code> <p>Raises:</p> Type Description <code>MissingArgumentsError</code> <p>Raised when <code>src_emb_from_pretrained</code> is supplied in <code>kwargs</code>, but <code>src_emb_pretrained_type</code> and <code>src_emb_pretrained_path</code> are not supplied. This error also applies for <code>tgt_emb_from_pretrained</code>.</p> <code>MissingArgumentsGloVeError</code> <p>Raised when GloVe embeddings from pretrained are wanted to be loaded and <code>src_tokenizer</code> is not supplied. This error also applies for <code>tgt_emb_from_pretrained</code>.</p> Source code in <code>src/tfs_mt/architecture.py</code> <pre><code>class Transformer(nn.Module):\n    \"\"\"Transformer model.\n\n    Using Language Model head to map decoder output representation to tokens in vocabulary.\n\n    Args:\n        src_vocab_size (int): Size of source language vocabulary.\n        tgt_vocab_size (int): Size of target language vocabulary.\n        num_encoder_blocks (int, optional): Number of encoder blocks. Defaults to 6.\n        num_decoder_blocks (int, optional): Number of decoder blocks. Defaults to 6.\n        d_model (int, optional): Model dimension. Defaults to 512.\n        num_heads (int, optional): Number of heads in MultiHead Attention. Defaults to 8.\n        d_ff (int, optional): Size of middle feedforward layer. Defaults to 2048.\n        norm_type (str, optional): Layer normalization strategy. Defaults to \"postnorm\".\n        dropout_prob (float, optional): Dropout probability. Defaults to 0.1.\n        max_seq_len (int, optional): Max sequence length. Defaults to 128.\n\n    Raises:\n        MissingArgumentsError: Raised when `src_emb_from_pretrained` is supplied in `kwargs`, but `src_emb_pretrained_type` and `src_emb_pretrained_path` are not supplied. This error also applies for `tgt_emb_from_pretrained`.\n        MissingArgumentsGloVeError: Raised when GloVe embeddings from pretrained are wanted to be loaded and `src_tokenizer` is not supplied. This error also applies for `tgt_emb_from_pretrained`.\n    \"\"\"\n\n    def __init__(\n        self,\n        src_vocab_size: int,\n        tgt_vocab_size: int,\n        num_encoder_blocks: int = 6,\n        num_decoder_blocks: int = 6,\n        d_model: int = 512,\n        num_heads: int = 8,\n        d_ff: int = 2048,\n        norm_type: str = \"postnorm\",\n        dropout_prob: float = 0.1,\n        max_seq_len: int = 128,\n        **kwargs,\n    ):\n        super().__init__()\n\n        # Source embedding init\n        if kwargs.get(\"src_emb_from_pretrained\") is not None:\n            if \"src_emb_pretrained_type\" not in kwargs or \"src_emb_pretrained_path\" not in kwargs:\n                raise MissingArgumentsError(\"src\")\n            if \"src_tokenizer\" not in kwargs and kwargs[\"src_emb_pretrained_type\"] == \"GloVe\":\n                raise MissingArgumentsGloVeError(\"src\")\n            self.src_embeddings = Embedding(\n                src_vocab_size,\n                d_model,\n                from_pretrained=True,\n                pretrained_emb_type=kwargs[\"src_emb_pretrained_type\"],\n                pretrained_emb_path=kwargs[\"src_emb_pretrained_path\"],\n                tokenizer=kwargs[\"src_tokenizer\"],\n            )\n        else:\n            self.src_embeddings = Embedding(src_vocab_size, d_model)\n\n        # Target embedding init\n        if kwargs.get(\"tgt_emb_from_pretrained\") is not None:\n            if \"tgt_emb_pretrained_type\" not in kwargs or \"tgt_emb_pretrained_path\" not in kwargs:\n                raise MissingArgumentsError(\"tgt\")\n            if \"tgt_tokenizer\" not in kwargs and kwargs[\"tgt_emb_pretrained_type\"] == \"GloVe\":\n                raise MissingArgumentsGloVeError(\"tgt\")\n            self.tgt_embeddings = Embedding(\n                tgt_vocab_size,\n                d_model,\n                from_pretrained=True,\n                pretrained_emb_type=kwargs[\"tgt_emb_pretrained_type\"],\n                pretrained_emb_path=kwargs[\"tgt_emb_pretrained_path\"],\n                tokenizer=kwargs[\"tgt_tokenizer\"],\n            )\n        else:\n            self.tgt_embeddings = Embedding(tgt_vocab_size, d_model)\n\n        self.src_pos_embeddings = SinusoidalPositionalEncoding(d_model, dropout_prob, max_sequence_length=max_seq_len)\n        self.tgt_pos_embeddings = SinusoidalPositionalEncoding(d_model, dropout_prob, max_sequence_length=max_seq_len)\n\n        self.encoder = nn.ModuleList([\n            EncoderBlock(d_model, num_heads, d_ff, dropout_prob, norm_type) for _ in range(num_encoder_blocks)\n        ])\n        self.decoder = nn.ModuleList([\n            DecoderBlock(d_model, num_heads, d_ff, dropout_prob, norm_type) for _ in range(num_decoder_blocks)\n        ])\n\n        self.unembedding_matrix = nn.Linear(d_model, tgt_vocab_size, bias=False)\n\n    def init_params(self):\n        \"\"\"Xavier initialize uninitialized layers.\n\n        This weight initialization strategy was first introduced [here](https://proceedings.mlr.press/v9/glorot10a.html). It's used to stabilize gradients during training.\n        \"\"\"\n        for name, p in self.named_parameters():\n            if \"embeddings\" in name:  # Embeddings are initialized in their init function\n                continue\n            if p.dim() &gt; 1:\n                nn.init.xavier_uniform_(p)\n\n    def forward(\n        self,\n        src_sequence: Float[torch.Tensor, \"B S\"],\n        tgt_sequence: Float[torch.Tensor, \"B S\"],\n        src_mask: Bool[torch.Tensor, \"B S\"],\n        tgt_mask: Bool[torch.Tensor, \"B S\"],\n    ) -&gt; Float[torch.Tensor, \"B S D\"]:\n        encoder_representation = self.encode(src_sequence, src_mask)\n        decoder_output = self.decode(tgt_sequence, encoder_representation, tgt_mask, src_mask)\n\n        return decoder_output\n\n    def encode(\n        self, src_sequence: Float[torch.Tensor, \"B S\"], src_mask: Bool[torch.Tensor, \"B S\"]\n    ) -&gt; Float[torch.Tensor, \"B S D\"]:\n        \"\"\"Encode method.\"\"\"\n        src_x = self.src_embeddings(src_sequence)\n        src_x = self.src_pos_embeddings(src_x)\n\n        # Reshape from [B, S] to [B, 1, 1, S] to properly broadcast attention mask to all QKt matrices\n        # Broadcasting doc: https://docs.pytorch.org/docs/stable/notes/broadcasting.html\n        src_mask = src_mask.unsqueeze(1).unsqueeze(2)\n\n        # Build attention mask matrix with shape [B, 1, S, S] to properly mask QKt matrix\n        # eg. with considering only the last 2 dimensions\n        # input = [[ True,  True, False]]\n        # output = [[ True,  True, False],\n        #           [ True,  True, False],\n        #           [False, False, False]]\n        src_mask = src_mask.to(torch.float)  # Convert to float to allow transpose operation\n        src_mask = torch.matmul(src_mask.transpose(-1, -2), src_mask)\n        src_mask = src_mask.to(torch.bool)  # Convert back to boolean mask\n\n        # Store mask for visualization purposes\n        self.attention_mask = src_mask\n\n        encoder_representation = src_x\n        for i in range(len(self.encoder)):\n            encoder_representation = self.encoder[i](encoder_representation, src_mask)\n\n        return encoder_representation\n\n    def decode(\n        self,\n        tgt_sequence: Float[torch.Tensor, \"B S\"],\n        encoder_representation: Float[torch.Tensor, \"B S D\"],\n        tgt_mask: Bool[torch.Tensor, \"B S\"],\n        src_mask: Bool[torch.Tensor, \"B S\"],\n    ) -&gt; Float[torch.Tensor, \"B S D\"]:\n        \"\"\"Decode method.\"\"\"\n        tgt_x = self.tgt_embeddings(tgt_sequence)\n        tgt_x = self.tgt_pos_embeddings(tgt_x)\n\n        tgt_mask = tgt_mask.unsqueeze(1).unsqueeze(2)\n        cross_src_mask = src_mask.unsqueeze(1).unsqueeze(2)\n\n        # Build cross attention mask matrix with shape [B, 1, S, S] to properly mask QKt matrix\n        # As done before in the encoder attention, now there's the need avoid attention computation to target pad tokens\n        # Please refer to the documentation for more details\n        # eg. with considering only the last 2 dimensions\n        # src_input = [[ True,  True, True, False]]\n        # tgt_input = [[ True,  True, False, False]]\n        # output = [[ True,  True, True, False],\n        #           [ True,  True, True, False],\n        #           [False, False, False, False],\n        #           [False, False, False, False]]\n        cross_src_mask = cross_src_mask.to(torch.float)  # Convert to float to allow transpose operation\n        tgt_mask = tgt_mask.to(torch.float)  # Convert to float to allow transpose operation\n        cross_src_mask = torch.matmul(tgt_mask.transpose(-1, -2), cross_src_mask)\n        cross_src_mask = cross_src_mask.to(torch.bool)  # Convert back to boolean mask\n\n        # Store mask for visualization purposes\n        self.cross_attention_mask = cross_src_mask\n\n        tgt_mask = torch.matmul(tgt_mask.transpose(-1, -2), tgt_mask)\n        tgt_mask = tgt_mask.to(torch.bool)\n\n        # Apply causal masking\n        # This speeds up computation since only one masked_fill will be applied in each decoder attention module\n        causal_mask = (\n            torch.triu(torch.ones((tgt_mask.shape[0], 1, tgt_mask.shape[-1], tgt_mask.shape[-1])), diagonal=1) == 0\n        ).to(tgt_mask.device)\n        tgt_mask = tgt_mask &amp; causal_mask  # Extract intersection between pad_mask and causal mask\n\n        # Store mask for visualization purposes\n        self.causal_attention_mask = tgt_mask\n\n        decoder_representation = tgt_x\n        for i in range(len(self.decoder)):\n            decoder_representation = self.decoder[i](\n                decoder_representation, encoder_representation, tgt_mask, cross_src_mask\n            )\n\n        # Language modeling head\n        # Decoded output represents the output logits, softmax needs to be applied if output probabilities are needed\n        decoder_output = self.unembedding_matrix(decoder_representation)\n\n        return decoder_output\n</code></pre>"},{"location":"modules/architecture/#src.tfs_mt.architecture.Transformer.decode","title":"<code>decode(tgt_sequence, encoder_representation, tgt_mask, src_mask)</code>","text":"<p>Decode method.</p> Source code in <code>src/tfs_mt/architecture.py</code> <pre><code>def decode(\n    self,\n    tgt_sequence: Float[torch.Tensor, \"B S\"],\n    encoder_representation: Float[torch.Tensor, \"B S D\"],\n    tgt_mask: Bool[torch.Tensor, \"B S\"],\n    src_mask: Bool[torch.Tensor, \"B S\"],\n) -&gt; Float[torch.Tensor, \"B S D\"]:\n    \"\"\"Decode method.\"\"\"\n    tgt_x = self.tgt_embeddings(tgt_sequence)\n    tgt_x = self.tgt_pos_embeddings(tgt_x)\n\n    tgt_mask = tgt_mask.unsqueeze(1).unsqueeze(2)\n    cross_src_mask = src_mask.unsqueeze(1).unsqueeze(2)\n\n    # Build cross attention mask matrix with shape [B, 1, S, S] to properly mask QKt matrix\n    # As done before in the encoder attention, now there's the need avoid attention computation to target pad tokens\n    # Please refer to the documentation for more details\n    # eg. with considering only the last 2 dimensions\n    # src_input = [[ True,  True, True, False]]\n    # tgt_input = [[ True,  True, False, False]]\n    # output = [[ True,  True, True, False],\n    #           [ True,  True, True, False],\n    #           [False, False, False, False],\n    #           [False, False, False, False]]\n    cross_src_mask = cross_src_mask.to(torch.float)  # Convert to float to allow transpose operation\n    tgt_mask = tgt_mask.to(torch.float)  # Convert to float to allow transpose operation\n    cross_src_mask = torch.matmul(tgt_mask.transpose(-1, -2), cross_src_mask)\n    cross_src_mask = cross_src_mask.to(torch.bool)  # Convert back to boolean mask\n\n    # Store mask for visualization purposes\n    self.cross_attention_mask = cross_src_mask\n\n    tgt_mask = torch.matmul(tgt_mask.transpose(-1, -2), tgt_mask)\n    tgt_mask = tgt_mask.to(torch.bool)\n\n    # Apply causal masking\n    # This speeds up computation since only one masked_fill will be applied in each decoder attention module\n    causal_mask = (\n        torch.triu(torch.ones((tgt_mask.shape[0], 1, tgt_mask.shape[-1], tgt_mask.shape[-1])), diagonal=1) == 0\n    ).to(tgt_mask.device)\n    tgt_mask = tgt_mask &amp; causal_mask  # Extract intersection between pad_mask and causal mask\n\n    # Store mask for visualization purposes\n    self.causal_attention_mask = tgt_mask\n\n    decoder_representation = tgt_x\n    for i in range(len(self.decoder)):\n        decoder_representation = self.decoder[i](\n            decoder_representation, encoder_representation, tgt_mask, cross_src_mask\n        )\n\n    # Language modeling head\n    # Decoded output represents the output logits, softmax needs to be applied if output probabilities are needed\n    decoder_output = self.unembedding_matrix(decoder_representation)\n\n    return decoder_output\n</code></pre>"},{"location":"modules/architecture/#src.tfs_mt.architecture.Transformer.encode","title":"<code>encode(src_sequence, src_mask)</code>","text":"<p>Encode method.</p> Source code in <code>src/tfs_mt/architecture.py</code> <pre><code>def encode(\n    self, src_sequence: Float[torch.Tensor, \"B S\"], src_mask: Bool[torch.Tensor, \"B S\"]\n) -&gt; Float[torch.Tensor, \"B S D\"]:\n    \"\"\"Encode method.\"\"\"\n    src_x = self.src_embeddings(src_sequence)\n    src_x = self.src_pos_embeddings(src_x)\n\n    # Reshape from [B, S] to [B, 1, 1, S] to properly broadcast attention mask to all QKt matrices\n    # Broadcasting doc: https://docs.pytorch.org/docs/stable/notes/broadcasting.html\n    src_mask = src_mask.unsqueeze(1).unsqueeze(2)\n\n    # Build attention mask matrix with shape [B, 1, S, S] to properly mask QKt matrix\n    # eg. with considering only the last 2 dimensions\n    # input = [[ True,  True, False]]\n    # output = [[ True,  True, False],\n    #           [ True,  True, False],\n    #           [False, False, False]]\n    src_mask = src_mask.to(torch.float)  # Convert to float to allow transpose operation\n    src_mask = torch.matmul(src_mask.transpose(-1, -2), src_mask)\n    src_mask = src_mask.to(torch.bool)  # Convert back to boolean mask\n\n    # Store mask for visualization purposes\n    self.attention_mask = src_mask\n\n    encoder_representation = src_x\n    for i in range(len(self.encoder)):\n        encoder_representation = self.encoder[i](encoder_representation, src_mask)\n\n    return encoder_representation\n</code></pre>"},{"location":"modules/architecture/#src.tfs_mt.architecture.Transformer.init_params","title":"<code>init_params()</code>","text":"<p>Xavier initialize uninitialized layers.</p> <p>This weight initialization strategy was first introduced here. It's used to stabilize gradients during training.</p> Source code in <code>src/tfs_mt/architecture.py</code> <pre><code>def init_params(self):\n    \"\"\"Xavier initialize uninitialized layers.\n\n    This weight initialization strategy was first introduced [here](https://proceedings.mlr.press/v9/glorot10a.html). It's used to stabilize gradients during training.\n    \"\"\"\n    for name, p in self.named_parameters():\n        if \"embeddings\" in name:  # Embeddings are initialized in their init function\n            continue\n        if p.dim() &gt; 1:\n            nn.init.xavier_uniform_(p)\n</code></pre>"},{"location":"modules/architecture/#src.tfs_mt.architecture.build_model","title":"<code>build_model(config, src_tokenizer=None, tgt_tokenizer=None, from_pretrained=False, **kwargs)</code>","text":"<p>Build Transformer model according to a config file.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>DictConfig | ListConfig | str</code> <p>Project config object or str to load it.</p> required <code>src_tokenizer</code> <code>BaseTokenizer | None</code> <p>Source text tokenizer. Defaults to None.</p> <code>None</code> <code>tgt_tokenizer</code> <code>BaseTokenizer | None</code> <p>Target text tokenizer. Defaults to None.</p> <code>None</code> <code>from_pretrained</code> <code>bool | None</code> <p>Load pretrained weights. Defaults to False.</p> <code>False</code> <p>Raises:</p> Type Description <code>ModelSizeNotChoosen</code> <p>Raised when config doesn't have <code>chosen_model_size</code> key.</p> <p>Returns:</p> Name Type Description <code>Transformer</code> <code>Transformer</code> <p>Initialized Transformer model according to config yaml file and choosen model size.</p> Source code in <code>src/tfs_mt/architecture.py</code> <pre><code>def build_model(\n    config: DictConfig | ListConfig | str,\n    src_tokenizer: BaseTokenizer | None = None,\n    tgt_tokenizer: BaseTokenizer | None = None,\n    from_pretrained: bool | None = False,\n    **kwargs,\n) -&gt; Transformer:\n    \"\"\"Build Transformer model according to a config file.\n\n    Args:\n        config (DictConfig | ListConfig | str): Project config object or str to load it.\n        src_tokenizer (BaseTokenizer | None, optional): Source text tokenizer. Defaults to None.\n        tgt_tokenizer (BaseTokenizer | None, optional): Target text tokenizer. Defaults to None.\n        from_pretrained (bool | None, optional): Load pretrained weights. Defaults to False.\n\n    Raises:\n        ModelSizeNotChoosen: Raised when config doesn't have `chosen_model_size` key.\n\n    Returns:\n        Transformer: Initialized Transformer model according to config yaml file and choosen model size.\n    \"\"\"\n\n    # Load config\n    if isinstance(config, str):\n        from omegaconf import OmegaConf\n\n        if config.startswith((\"https://\", \"http://\")):\n            from io import StringIO\n\n            if not config.endswith((\".yaml\", \".yml\")):\n                Exception(\n                    f\"File extension not supported: `{config.split('.')[-1]}`. Please load a `.yaml` or `.yml` file.\"\n                )\n            try:\n                resp = requests.get(config, timeout=10)\n                resp.raise_for_status()\n                config = OmegaConf.load(StringIO(resp.text))\n            except Exception as e:\n                raise RuntimeError(f\"Failed to download config file from {config}: {e}\") from e\n        else:\n            if os.path.exists(config) and config.endswith((\".yaml\", \".yml\")):\n                config = OmegaConf.load(config)\n            else:\n                raise FileNotFoundError(\n                    \"Config yaml file not found or wrong file provided. Only `.yaml` and `.yml` are allowed.\"\n                )\n\n    if \"chosen_model_size\" not in config:\n        raise ModelSizeNotChoosen()\n\n    # Load from pretrained\n    if from_pretrained:\n        if kwargs.get(\"model_path\") is not None:\n            model = Transformer(\n                src_vocab_size=config.src_tokenizer_vocab_size,\n                tgt_vocab_size=config.tgt_tokenizer_vocab_size,\n                num_encoder_blocks=config.model_configs[config.chosen_model_size].num_encoder_layers,\n                num_decoder_blocks=config.model_configs[config.chosen_model_size].num_decoder_layers,\n                d_model=config.model_configs[config.chosen_model_size].d_model,\n                num_heads=config.model_configs[config.chosen_model_size].num_heads,\n                d_ff=config.model_configs[config.chosen_model_size].d_ff,\n                norm_type=config.model_configs[config.chosen_model_size].norm_type,\n                dropout_prob=config.model_parameters.dropout,\n                max_seq_len=config.tokenizer.max_seq_len,\n            )\n\n            model_filepath = None\n            if str(kwargs[\"model_path\"]).startswith((\"https://\", \"http://\")):\n                try:\n                    response = requests.get(kwargs[\"model_path\"], timeout=100)\n                    response.raise_for_status()\n\n                    if kwargs[\"model_path\"].endswith(\".safetensors\"):\n                        suffix = \".safetensors\"\n                    elif kwargs[\"model_path\"].endswith(\".pt\"):\n                        suffix = \".pt\"\n                    elif kwargs[\"model_path\"].endswith(\".pth\"):\n                        suffix = \".pth\"\n                    else:\n                        raise InvalidFileExtensionException(kwargs[\"model_path\"].split(\".\")[-1])\n\n                    with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp_file:\n                        tmp_file.write(response.content)\n                        model_filepath = tmp_file.name\n\n                except Exception as e:\n                    raise RuntimeError(f\"Failed to download model from {kwargs['model_path']}: {e}\") from e\n            else:\n                model_filepath = kwargs[\"model_path\"]\n\n            # Load the model weights\n            try:\n                if model_filepath.endswith(\".safetensors\"):\n                    state_dict = load_file(model_filepath)\n                else:\n                    state_dict = torch.load(model_filepath, map_location=\"cpu\", weights_only=True)\n                    first_layer_key = next(iter(state_dict.keys()))\n                    if first_layer_key.startswith(\"_orig_mod.\"):\n                        new_state_dict = {}\n                        prefix = \"_orig_mod.\"\n                        for k, v in state_dict.items():\n                            new_k = k[len(prefix) :] if k.startswith(prefix) else k\n                            new_state_dict[new_k] = v\n                        state_dict = new_state_dict\n\n                model.load_state_dict(state_dict, strict=False, assign=True)\n\n                if str(kwargs[\"model_path\"]).startswith((\"https://\", \"http://\")):\n                    os.unlink(model_filepath)\n            except Exception as e:\n                raise RuntimeError(f\"Failed to load model weights from {model_filepath}: {e}\") from e\n\n            return model\n\n        else:\n            raise MissingArgumentsError(\"Missing model_path argument\")\n\n    # Disable pretrained_word_embeddings option when no GloVe embeddings version and filename are provided in the configuration\n    if (\n        config.model_configs[config.chosen_model_size].get(\"glove_version\", None) is None\n        and config.model_configs[config.chosen_model_size].get(\"glove_filename\", None) is None\n    ):\n        config.model_configs.pretrained_word_embeddings = None\n\n    # NOTE GloVe embeddings available only for English language\n    if (\n        config.dataset.src_lang != \"en\" and config.dataset.tgt_lang != \"en\"\n    ) or config.model_configs.pretrained_word_embeddings is None:\n        model = Transformer(\n            src_vocab_size=src_tokenizer.vocab_size,\n            tgt_vocab_size=tgt_tokenizer.vocab_size,\n            num_encoder_blocks=config.model_configs[config.chosen_model_size].num_encoder_layers,\n            num_decoder_blocks=config.model_configs[config.chosen_model_size].num_decoder_layers,\n            d_model=config.model_configs[config.chosen_model_size].d_model,\n            num_heads=config.model_configs[config.chosen_model_size].num_heads,\n            d_ff=config.model_configs[config.chosen_model_size].d_ff,\n            norm_type=config.model_configs[config.chosen_model_size].norm_type,\n            dropout_prob=config.model_parameters.dropout,\n            max_seq_len=config.tokenizer.max_seq_len,\n        )\n        model.init_params()\n\n    else:\n        glove_version = config.model_configs[config.chosen_model_size].glove_version\n        glove_filename = config.model_configs[config.chosen_model_size].glove_filename\n\n        glove_path = os.path.join(config.base_path, f\"data/{glove_version}/{glove_filename}.txt\")\n\n        if config.dataset.src_lang == \"en\":  # English is the source language\n            model = Transformer(\n                src_vocab_size=src_tokenizer.vocab_size,\n                tgt_vocab_size=tgt_tokenizer.vocab_size,\n                num_encoder_blocks=config.model_configs[config.chosen_model_size].num_encoder_layers,\n                num_decoder_blocks=config.model_configs[config.chosen_model_size].num_decoder_layers,\n                d_model=config.model_configs[config.chosen_model_size].d_model,\n                num_heads=config.model_configs[config.chosen_model_size].num_heads,\n                d_ff=config.model_configs[config.chosen_model_size].d_ff,\n                norm_type=config.model_configs[config.chosen_model_size].norm_type,\n                dropout_prob=config.model_parameters.dropout,\n                max_seq_len=config.tokenizer.max_seq_len,\n                src_emb_from_pretrained=True,\n                src_emb_pretrained_type=config.model_configs.pretrained_word_embeddings,\n                src_emb_pretrained_path=glove_path,\n                src_tokenizer=src_tokenizer,\n            )\n\n        else:  # English is the target language\n            model = Transformer(\n                src_vocab_size=src_tokenizer.vocab_size,\n                tgt_vocab_size=tgt_tokenizer.vocab_size,\n                num_encoder_blocks=config.model_configs[config.chosen_model_size].num_encoder_layers,\n                num_decoder_blocks=config.model_configs[config.chosen_model_size].num_decoder_layers,\n                d_model=config.model_configs[config.chosen_model_size].d_model,\n                num_heads=config.model_configs[config.chosen_model_size].num_heads,\n                d_ff=config.model_configs[config.chosen_model_size].d_ff,\n                norm_type=config.model_configs[config.chosen_model_size].norm_type,\n                dropout_prob=config.model_parameters.dropout,\n                max_seq_len=config.tokenizer.max_seq_len,\n                tgt_emb_from_pretrained=True,\n                tgt_emb_pretrained_type=config.model_configs.pretrained_word_embeddings,\n                tgt_emb_pretrained_path=glove_path,\n                tgt_tokenizer=tgt_tokenizer,\n            )\n\n        model.init_params()\n\n    return model\n</code></pre>"},{"location":"modules/data_utils/","title":"Data utils","text":""},{"location":"modules/data_utils/#src.tfs_mt.data_utils.TranslationDataset","title":"<code>TranslationDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>Translation Dataset.</p> <p>Parameters:</p> Name Type Description Default <code>src_texts</code> <code>list[str]</code> <p>List of source texts.</p> required <code>tgt_texts</code> <code>list[str]</code> <p>List of target texts.</p> required <code>src_tokenizer</code> <code>BaseTokenizer</code> <p>Tokenizer used to preprocess the source language text.</p> required <code>tgt_tokenizer</code> <code>BaseTokenizer</code> <p>Tokenizer used to preprocess the target language text.</p> required <code>src_lang</code> <code>str</code> <p>Identifier for the source language, e.g., <code>\"en\"</code> for English.</p> required <code>tgt_lang</code> <code>str</code> <p>Identifier for the target language, e.g., <code>\"it\"</code> for Italian.</p> required <code>max_sequence_length</code> <code>int | None</code> <p>Maximum sequence length for tokenization. If None, sequences are not truncated. Defaults to None.</p> <code>None</code> Source code in <code>src/tfs_mt/data_utils.py</code> <pre><code>class TranslationDataset(Dataset):\n    \"\"\"Translation Dataset.\n\n    Args:\n        src_texts (list[str]): List of source texts.\n        tgt_texts (list[str]): List of target texts.\n        src_tokenizer (BaseTokenizer): Tokenizer used to preprocess the source language text.\n        tgt_tokenizer (BaseTokenizer): Tokenizer used to preprocess the target language text.\n        src_lang (str): Identifier for the source language, e.g., `\"en\"` for English.\n        tgt_lang (str): Identifier for the target language, e.g., `\"it\"` for Italian.\n        max_sequence_length (int | None, optional): Maximum sequence length for tokenization. If None, sequences are not truncated. Defaults to None.\n    \"\"\"\n\n    def __init__(\n        self,\n        src_texts: list[str],\n        tgt_texts: list[str],\n        src_tokenizer: BaseTokenizer,\n        tgt_tokenizer: BaseTokenizer,\n        src_lang: str,\n        tgt_lang: str,\n        max_sequence_length: int | None = None,\n        **kwargs,\n    ):\n        self.src_tokenizer = src_tokenizer\n        self.tgt_tokenizer = tgt_tokenizer\n        self.src_lang = src_lang\n        self.tgt_lang = tgt_lang\n        self.max_sequence_length = max_sequence_length\n\n        # Build tokenizers vocab if empty\n        if self.src_tokenizer.vocab_size == 0 or self.tgt_tokenizer.vocab_size == 0:\n            if \"extend_vocab_with_glove\" in kwargs and \"glove_version\" in kwargs:\n                self._build_vocabs(\n                    vocab_min_freq=kwargs.get(\"vocab_min_freq\", 2),\n                    extend_with_glove=kwargs.get(\"extend_vocab_with_glove\", True),\n                    glove_version=kwargs.get(\"glove_version\", \"glove.2024.wikigiga.50d\"),\n                )\n            else:\n                self._build_vocabs(kwargs.get(\"vocab_min_freq\", 2))\n\n        # Filter input data excluding texts longer than max_sequence_length\n        if max_sequence_length is not None and max_sequence_length &gt; 0:\n            print(f\"Max sequence length set to {max_sequence_length}.\")\n            self.src_texts, self.tgt_texts = [], []\n        else:\n            self.src_texts = src_texts\n            self.tgt_texts = tgt_texts\n\n        # Prepare tokenized texts here to have them ready in __getitem__ method\n        # This will speed up the batch preparation in the dataloader but it will raise system memory usage since the tokenized sequences are cached as dataset attribute\n        # (The masks are also cached but their impact is negligible since they are arrays of boolean values)\n        print(\"Caching encoded sequences in memory, this may take some time...\")\n        self.src_encoded_sequences, self.tgt_encoded_sequences = [], []\n        self.src_masks, self.tgt_masks = [], []\n        for src_text, tgt_text in zip(src_texts, tgt_texts, strict=False):\n            src_text_tokenized = src_tokenizer.tokenize(src_text)\n            tgt_text_tokenized = tgt_tokenizer.tokenize(tgt_text)\n\n            # Exclude sequences that exceed max_sequence_length\n            if max_sequence_length is not None and max_sequence_length &gt; 0:\n                if (\n                    len(src_text_tokenized) &gt; max_sequence_length - 2\n                ):  # -2 accounts for SOS and EOS tokens that will be added in the __getitem__ method\n                    continue\n                if len(tgt_text_tokenized) &gt; max_sequence_length - 2:\n                    continue\n                self.src_texts.append(src_text)\n                self.tgt_texts.append(tgt_text)\n\n            # src and tgt sequence lengths must be the same to properly compute cross attention\n            # The smaller sequence will be padded to the length of the longer sequence\n            # Attention mask ensure no attention is computed with pad tokens\n            max_seq_len = max(len(src_text_tokenized), len(tgt_text_tokenized))\n\n            # Tokenize texts\n            src_tokens, src_mask = self.src_tokenizer.encode(src_text_tokenized, pad_to_len=max_seq_len)\n            tgt_tokens, tgt_mask = self.tgt_tokenizer.encode(tgt_text_tokenized, pad_to_len=max_seq_len)\n\n            self.src_encoded_sequences.append(src_tokens)\n            self.tgt_encoded_sequences.append(tgt_tokens)\n            self.src_masks.append(src_mask)\n            self.tgt_masks.append(tgt_mask)\n\n    def _build_vocabs(self, vocab_min_freq: int = 2, extend_with_glove: bool = False, **kwargs) -&gt; None:\n        \"\"\"Build vocabularies for tokenizers.\"\"\"\n\n        print(\"Building vocabs, it may take a few minutes...\")\n\n        # Provides lists of tokens. Here the lists are not converted to sets cause the tokenizer may need the token frequencies\n        src_tokens = [token for text in self.src_texts for token in self.src_tokenizer.tokenize(text)]\n        tgt_tokens = [token for text in self.tgt_texts for token in self.tgt_tokenizer.tokenize(text)]\n\n        self.src_tokenizer.build_vocab(\n            src_tokens,\n            min_freq=vocab_min_freq,\n            extend_with_glove=bool(\n                extend_with_glove and self.src_lang == \"en\"\n            ),  # GloVe is trained on english only datasets so it doesn't make sense to extend non english vocabs\n            glove_version=kwargs.get(\"glove_version\", \"glove.2024.wikigiga.50d\"),\n        )\n        self.tgt_tokenizer.build_vocab(\n            tgt_tokens,\n            min_freq=vocab_min_freq,\n            extend_with_glove=bool(extend_with_glove and self.tgt_lang == \"en\"),\n            glove_version=kwargs.get(\"glove_version\", \"glove.2024.wikigiga.50d\"),\n        )\n\n    def __len__(self):\n        return len(self.src_texts)\n\n    def __getitem__(self, idx: int) -&gt; dict[str, torch.Tensor | str]:\n        src_tokens = self.src_encoded_sequences[idx]\n        tgt_tokens = self.tgt_encoded_sequences[idx]\n        src_mask = self.src_masks[idx]\n        tgt_mask = self.tgt_masks[idx]\n        src_text = self.src_texts[idx]\n        tgt_text = self.tgt_texts[idx]\n\n        return {\n            \"src\": torch.tensor(src_tokens, dtype=torch.long),\n            \"tgt\": torch.tensor(tgt_tokens, dtype=torch.long),\n            \"src_mask\": torch.tensor(src_mask, dtype=torch.bool),\n            \"tgt_mask\": torch.tensor(tgt_mask, dtype=torch.bool),\n            \"src_text\": src_text,\n            \"tgt_text\": tgt_text,\n        }\n</code></pre>"},{"location":"modules/data_utils/#src.tfs_mt.data_utils.WordTokenizer","title":"<code>WordTokenizer</code>","text":"<p>               Bases: <code>BaseTokenizer</code></p> <p>Word tokenizer. Mainly used to let the model be compatible with pretrained GloVe embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>special_tokens</code> <code>dict[str, str] | None</code> <p>Special tokens to be considered, eg. SOS_TOKEN, EOS_TOKEN. Defaults to None.</p> <code>None</code> <code>contractions</code> <code>list[str] | None</code> <p>Contractions to be considered, eg. 's, 'll. If None the following set of contractions will be considered: <code>'s</code>, <code>'re</code>, <code>'ve</code>, <code>'m</code>, <code>'ll</code>, <code>'d</code>, <code>n't</code>, <code>'t</code>, <code>n'ts</code>. Defaults to None.</p> <code>None</code> <code>tokenizer_max_len</code> <code>int</code> <p>Tokenizer max sequence length. Mainly used to limit memory usage and performance impact during training and inference due to attention quadratic complexity. Defaults to 128.</p> <code>128</code> <code>max_vocab_size</code> <code>int</code> <p>Maximum number of token in vocabulary. Defaults to 100_000.</p> <code>100000</code> Source code in <code>src/tfs_mt/data_utils.py</code> <pre><code>class WordTokenizer(BaseTokenizer):\n    \"\"\"Word tokenizer.\n    Mainly used to let the model be compatible with pretrained GloVe embeddings.\n\n    Args:\n        special_tokens (dict[str, str] | None, optional): Special tokens to be considered, eg. SOS_TOKEN, EOS_TOKEN. Defaults to None.\n        contractions (list[str] | None, optional): Contractions to be considered, eg. 's, 'll. If None the following set of contractions will be considered: `'s`, `'re`, `'ve`, `'m`, `'ll`, `'d`, `n't`, `'t`, `n'ts`. Defaults to None.\n        tokenizer_max_len (int, optional): Tokenizer max sequence length. Mainly used to limit memory usage and performance impact during training and inference due to attention quadratic complexity. Defaults to 128.\n        max_vocab_size (int, optional): Maximum number of token in vocabulary. Defaults to 100_000.\n    \"\"\"\n\n    def __init__(\n        self,\n        special_tokens: dict[str, str] | None = None,\n        contractions: list[str, str] | None = None,\n        tokenizer_max_len: int = 128,\n        max_vocab_size: int = 100_000,\n    ):\n        self.vocab: dict[str, int] = {}\n        self.vocab_reverse: dict[int, str] = {}  # Useful for efficient decoding\n\n        self.tokenizer_max_len = tokenizer_max_len\n        self.max_vocab_size = max_vocab_size\n\n        self.special_tokens = special_tokens or {\n            \"sos_token\": \"&lt;s&gt;\",\n            \"eos_token\": \"&lt;/s&gt;\",\n            \"pad_token\": \"&lt;PAD&gt;\",\n            \"unk_token\": \"&lt;UNK&gt;\",\n        }\n\n        # Setup contractions management utilities\n        contractions_init = contractions or [\"'s\", \"'re\", \"'ve\", \"'m\", \"'ll\", \"'d\", \"n't\", \"'t\", \"n'ts\"]\n\n        # Prepare a dict of the following type to address the detection of nested contractions:\n        # nested_contractions_dict = {\n        #     \"ANYTHING_n't_ANYTHING\": [\"n't\"],\n        #     \"n't\": [\"'t\"]\n        # }\n        # NOTE \"'t\" is technically a sub contraction of both \"ANYTHING_n't_ANYTHING\" and \"n't\" but it will be only considered as sub contraction of n't\n        self.nested_contractions_dict: dict[str, list[str]] = {}\n\n        for contraction in contractions_init:\n            other_contractions = contractions_init.copy()\n            other_contractions.remove(contraction)\n            for comp_contraction in other_contractions:\n                if contraction in comp_contraction:  # Detect sub contraction\n                    # Create new mapping in dict\n                    if comp_contraction not in self.nested_contractions_dict:\n                        self.nested_contractions_dict[comp_contraction] = [contraction]\n                    # Detect sub contraction already present in the dict\n                    elif comp_contraction in self.nested_contractions_dict and contraction not in list(\n                        chain.from_iterable(self.nested_contractions_dict.values())\n                    ):\n                        # Merge all elements in sub contractions lists in a single list\n                        self.nested_contractions_dict[comp_contraction].append(contraction)\n\n        # Final sets of sub contractions and non-nested contractions\n        # NOTE the plain list of all sub contractions will be used during tokenizationin order to better detect correct splitting when the `'` is encountered\n        self.all_sub_contractions = list(chain.from_iterable(self.nested_contractions_dict.values()))\n        self.contractions = [c for c in contractions_init if c not in self.all_sub_contractions]\n\n        self.glove_available_versions = [\n            \"glove.2024.dolma.300d\",\n            \"glove.2024.wikigiga.300d\",\n            \"glove.2024.wikigiga.200d\",\n            \"glove.2024.wikigiga.100d\",\n            \"glove.2024.wikigiga.50d\",\n            \"glove.42B.300d\",\n            \"glove.6B\",\n            \"glove.840B.300d\",\n            \"glove.twitter.27B\",\n        ]\n\n    @property\n    def vocab_size(self):\n        return len(self.vocab.items()) if self.vocab else 0\n\n    @property\n    def sos_token_idx(self):\n        return self.vocab.get(self.special_tokens[\"sos_token\"], 0)\n\n    @property\n    def eos_token_idx(self):\n        return self.vocab.get(self.special_tokens[\"eos_token\"], 1)\n\n    @property\n    def pad_token_idx(self):\n        return self.vocab.get(self.special_tokens[\"pad_token\"], 2)\n\n    @property\n    def unk_token_idx(self):\n        return self.vocab.get(self.special_tokens[\"unk_token\"], 3)\n\n    @classmethod\n    def from_pretrained(cls: type[\"WordTokenizer\"], path: str) -&gt; \"WordTokenizer\":\n        if path.startswith((\"https://\", \"http://\")):\n            try:\n                response = requests.get(path, timeout=100)\n                response.raise_for_status()\n                data = response.json()\n            except Exception as e:\n                raise RuntimeError(f\"Failed to download tokenizer from {path}: {e}\") from e\n        else:\n            if os.path.exists(path):\n                with open(path, encoding=\"utf-8\") as f:\n                    data = json.load(f)\n\n        if data.get(\"vocab\") is None:\n            raise CheckpointEmptyVocabException()\n\n        tokenizer = cls(special_tokens=data.get(\"special_tokens\", {}), contractions=data.get(\"contractions\", {}))\n        tokenizer.vocab = data.get(\"vocab\")\n        tokenizer.vocab_reverse = {idx: token for token, idx in tokenizer.vocab.items()}\n\n        return tokenizer\n\n    def to_json(self, output_path: str) -&gt; None:\n        if self.vocab_size == 0:\n            raise VocabNotBuiltError()\n\n        to_save = {\n            \"type\": \"WordTokenizer\",\n            \"special_tokens\": self.special_tokens,\n            \"contractions\": self.contractions,\n            \"vocab\": self.vocab,\n        }\n\n        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n            json.dump(to_save, f, ensure_ascii=False, indent=2)\n\n    def build_vocab(\n        self,\n        tokens: list[str],\n        min_freq: int = 2,\n        extend_with_glove: bool = False,\n        glove_version: str = \"glove.2024.wikigiga.50d\",\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"Build vocabulary method.\n\n        Args:\n            tokens (list[str]): Tokens from dataset to build vocabulary on.\n            min_freq (int, optional): Minimum number of times a token has to appear in the dataset to be included in the vocabulary. Defaults to 2.\n            extend_with_glove (bool, optional): Enable vocabulary extension with GloVe tokens. Defaults to False.\n            glove_version (str, optional): GloVe version to use if `extend_with_glove` is `True`. Defaults to \"glove.2024.wikigiga.50d\".\n\n        Raises:\n            GloVeVersionError: Raised when supplied glove_version is unavailable.\n        \"\"\"\n        vocab_list = []\n        vocab_list.extend(self.special_tokens.values())\n        vocab_set = set(vocab_list)\n\n        if min_freq &gt; 1:\n            token_counts = Counter(tokens)\n            for token, count in token_counts.items():\n                # len(vocab_set) is O(1) operation cause the length is stored as a set attribute\n                if (count &gt;= min_freq and self.max_vocab_size == -1) or (\n                    count &gt;= min_freq and self.max_vocab_size &gt; 0 and len(vocab_set) &lt; self.max_vocab_size\n                ):\n                    vocab_set.add(token.lower())\n        else:\n            for token in tokens:\n                if self.max_vocab_size == -1 or (self.max_vocab_size &gt; 0 and len(vocab_set) &lt; self.max_vocab_size):\n                    vocab_set.add(token.lower())\n\n        vocab_list = list(vocab_set)\n        del vocab_set\n\n        if extend_with_glove and (\n            self.max_vocab_size == -1 or (self.max_vocab_size &gt; 0 and len(vocab_list) &lt; self.max_vocab_size)\n        ):\n            print(\"Extending vocab with GloVe tokens...\")\n\n            if glove_version not in self.glove_available_versions:\n                raise GloVeVersionError(glove_version, self.glove_available_versions)\n\n            data_path = os.getcwd() + \"/data\" if \"data_path\" not in kwargs else kwargs[\"data_path\"]\n\n            glove_tokens = []\n\n            try:\n                glove_filepath = download_glove(data_path, glove_version)\n\n                print(f\"Loading GloVe {glove_version} tokens from file...\")\n\n                with open(glove_filepath, encoding=\"utf-8\") as f:\n                    lines = f.readlines()\n\n                # Parse GloVe tokens\n                for line in lines:\n                    parts = line.strip().split()\n                    try:\n                        float(parts[1])\n                    except ValueError:\n                        continue\n                    else:\n                        token = parts[0].lower()\n                        glove_tokens.append(token)\n\n                initial_size = len(vocab_list)\n                if self.max_vocab_size != -1:\n                    for tok in glove_tokens:\n                        if tok not in vocab_list:\n                            vocab_list.append(tok)\n                        if len(vocab_list) == self.max_vocab_size:\n                            break\n                else:\n                    vocab_list.extend(glove_tokens)\n                    vocab_list = list(set(vocab_list))\n\n                print(f\"Added {len(vocab_list) - initial_size} tokens from GloVe\")\n\n            except Exception as e:\n                print(f\"Error with GloVe processing GloVe: {e}\")\n\n        # Create mappings\n        self.vocab = {token: idx for idx, token in enumerate(vocab_list)}\n        self.vocab_reverse = dict(enumerate(vocab_list))\n\n        del vocab_list\n\n        print(f\"Built vocabulary with {len(self.vocab.items())} tokens.\")\n\n    def tokenize(self, text: str) -&gt; list[str]:\n        \"\"\"Tokenizer based on GloVe word tokenizer in order to let the model be compatible with GloVe pretrained embeddings.\n\n        Max word length is 1000. Contractions are treated as distinct tokens, eg. `n't`, `'s`, `'ll`.\n\n        Reference: [GloVe source code](https://github.com/stanfordnlp/GloVe/blob/master/src/common.c#L75)\n\n        Args:\n            text (str): text to be tokenized.\n\n        Returns:\n            list[str]: List of string tokens from text.\n        \"\"\"\n\n        text = text.strip().lower()\n\n        for contraction in self.contractions:\n            pattern = r\"([a-zA-Z]+)\" + re.escape(contraction) + r\"\\b\"\n            text = re.sub(pattern, r\"\\1 \" + contraction, text)\n\n        for complete_contraction, sub_list in self.nested_contractions_dict.items():\n            pattern = r\"([a-zA-Z]+)\" + re.escape(complete_contraction) + r\"\\b\"\n            if not re.compile(complete_contraction).search(text):\n                for sub_contraction in sub_list:\n                    pattern = r\"([a-zA-Z]+)\" + re.escape(sub_contraction) + r\"\\b\"\n                    text = re.sub(pattern, r\"\\1 \" + sub_contraction, text)\n            else:\n                text = re.sub(pattern, r\"\\1 \" + complete_contraction, text)\n\n        words = text.split()\n\n        tokens = []\n        for word in words:\n            # Split words when encountering a `'` that's not involved in a quote or a contraction.\n            # eg. \"Quell'ultimo\" gets splitted into \"Quell'\" and \"ultimo\"\n            if (\n                \"'\" in word\n                and word[0] != \"'\"\n                and word[-1] != \"'\"\n                and word not in self.contractions\n                and word not in self.all_sub_contractions\n            ):\n                parts = word.split(\"'\")\n                for i, part in enumerate(parts):\n                    if part:\n                        if i &lt; len(parts) - 1:\n                            part += \"'\"\n                        tokens.append(part)\n\n            # Handle trailing punctuation\n            elif word and word[-1] in string.punctuation and word[-1] != \"'\":\n                # Strip all trailing punctuation\n                core_word = word.rstrip(string.punctuation)\n                punct = word[len(core_word) :]\n                if core_word:\n                    tokens.append(core_word)\n                for p in punct:\n                    tokens.append(p)\n\n            else:\n                if word:\n                    tokens.append(word)\n\n        # Truncate sequences by default\n        return tokens[: self.tokenizer_max_len]\n\n    def encode(\n        self,\n        input_sequence: str | list[str] | np.ndarray | torch.Tensor,\n        pad_to_len: int | None = None,\n        return_only_mask: bool = False,\n    ) -&gt; tuple[np.ndarray, np.ndarray]:\n        \"\"\"Tokenizer encode function.\n\n        It also returns the mask to be used during attention in order not compute it with respect to PAD tokens.\n        The mask is designed to True where there's a token with the model has to compute attention to, False otherwise.\n\n        Args:\n            input_sequence (str | list[str] | np.ndarray | torch.Tensor): Sequence to be encoded or already encoded (useful in decoding stage when this method is only used to provide the mask).\n            pad_to_len (int | None, optional): Sequence length to pad `input_sequence` with. Defaults to None.\n            return_only_mask (bool, optional): Return only attention mask. Defaults to False.\n\n        Raises:\n            VocabNotBuiltError: Raised when vocabulary is not built.\n\n        Returns:\n            tuple[np.ndarray, np.ndarray]: Array of token ids and mask.\n        \"\"\"\n\n        if self.vocab_size == 0:\n            raise VocabNotBuiltError()\n\n        if return_only_mask:\n            if isinstance(input_sequence, np.ndarray):\n                mask = input_sequence != self.pad_token_idx\n            elif isinstance(input_sequence, torch.Tensor):\n                mask = (input_sequence != self.pad_token_idx).to(input_sequence.device)\n            else:\n                raise ValueError()\n            return mask\n\n        # Useful when building a TranslationDataset in order to execute tokenize method only once\n        tokens = self.tokenize(input_sequence) if isinstance(input_sequence, str) else input_sequence\n\n        # Add SOS and EOS tokens to given sequence\n        if tokens[0] != self.special_tokens[\"sos_token\"]:\n            tokens.insert(0, self.special_tokens[\"sos_token\"])\n        if tokens[-1] != self.special_tokens[\"eos_token\"]:\n            tokens.append(self.special_tokens[\"eos_token\"])\n\n        token_ids = [self.vocab.get(token, self.unk_token_idx) for token in tokens]\n\n        if pad_to_len is not None:  # Pad sequence to pad_to_len\n            pad_to_len += 2  # Considering SOS and EOS tokens\n            token_ids.extend([self.pad_token_idx for _ in range(pad_to_len - len(tokens))])\n\n        token_ids = np.array(token_ids, dtype=np.long)\n\n        # Mask to disable attention to pad tokens\n        mask = np.array([token != self.pad_token_idx for token in token_ids], dtype=np.bool)\n\n        return token_ids, mask\n\n    def decode(self, token_ids: np.ndarray | list[str]) -&gt; list[str]:\n        \"\"\"Decode token IDs.\n        Returns the unknown token if the input token is not present in the vocabulary.\n\n        Args:\n            token_ids (np.ndarray | list[str]): Array or list of tokens ids to decode into text.\n\n        Raises:\n            VocabNotBuiltError: Vocabulary is not built.\n\n        Returns:\n            list[str]: Decoded text.\n        \"\"\"\n        if self.vocab_size == 0:\n            raise VocabNotBuiltError()\n        return [self.vocab_reverse.get(idx, self.special_tokens[\"unk_token\"]) for idx in token_ids]\n</code></pre>"},{"location":"modules/data_utils/#src.tfs_mt.data_utils.WordTokenizer.build_vocab","title":"<code>build_vocab(tokens, min_freq=2, extend_with_glove=False, glove_version='glove.2024.wikigiga.50d', **kwargs)</code>","text":"<p>Build vocabulary method.</p> <p>Parameters:</p> Name Type Description Default <code>tokens</code> <code>list[str]</code> <p>Tokens from dataset to build vocabulary on.</p> required <code>min_freq</code> <code>int</code> <p>Minimum number of times a token has to appear in the dataset to be included in the vocabulary. Defaults to 2.</p> <code>2</code> <code>extend_with_glove</code> <code>bool</code> <p>Enable vocabulary extension with GloVe tokens. Defaults to False.</p> <code>False</code> <code>glove_version</code> <code>str</code> <p>GloVe version to use if <code>extend_with_glove</code> is <code>True</code>. Defaults to \"glove.2024.wikigiga.50d\".</p> <code>'glove.2024.wikigiga.50d'</code> <p>Raises:</p> Type Description <code>GloVeVersionError</code> <p>Raised when supplied glove_version is unavailable.</p> Source code in <code>src/tfs_mt/data_utils.py</code> <pre><code>def build_vocab(\n    self,\n    tokens: list[str],\n    min_freq: int = 2,\n    extend_with_glove: bool = False,\n    glove_version: str = \"glove.2024.wikigiga.50d\",\n    **kwargs,\n) -&gt; None:\n    \"\"\"Build vocabulary method.\n\n    Args:\n        tokens (list[str]): Tokens from dataset to build vocabulary on.\n        min_freq (int, optional): Minimum number of times a token has to appear in the dataset to be included in the vocabulary. Defaults to 2.\n        extend_with_glove (bool, optional): Enable vocabulary extension with GloVe tokens. Defaults to False.\n        glove_version (str, optional): GloVe version to use if `extend_with_glove` is `True`. Defaults to \"glove.2024.wikigiga.50d\".\n\n    Raises:\n        GloVeVersionError: Raised when supplied glove_version is unavailable.\n    \"\"\"\n    vocab_list = []\n    vocab_list.extend(self.special_tokens.values())\n    vocab_set = set(vocab_list)\n\n    if min_freq &gt; 1:\n        token_counts = Counter(tokens)\n        for token, count in token_counts.items():\n            # len(vocab_set) is O(1) operation cause the length is stored as a set attribute\n            if (count &gt;= min_freq and self.max_vocab_size == -1) or (\n                count &gt;= min_freq and self.max_vocab_size &gt; 0 and len(vocab_set) &lt; self.max_vocab_size\n            ):\n                vocab_set.add(token.lower())\n    else:\n        for token in tokens:\n            if self.max_vocab_size == -1 or (self.max_vocab_size &gt; 0 and len(vocab_set) &lt; self.max_vocab_size):\n                vocab_set.add(token.lower())\n\n    vocab_list = list(vocab_set)\n    del vocab_set\n\n    if extend_with_glove and (\n        self.max_vocab_size == -1 or (self.max_vocab_size &gt; 0 and len(vocab_list) &lt; self.max_vocab_size)\n    ):\n        print(\"Extending vocab with GloVe tokens...\")\n\n        if glove_version not in self.glove_available_versions:\n            raise GloVeVersionError(glove_version, self.glove_available_versions)\n\n        data_path = os.getcwd() + \"/data\" if \"data_path\" not in kwargs else kwargs[\"data_path\"]\n\n        glove_tokens = []\n\n        try:\n            glove_filepath = download_glove(data_path, glove_version)\n\n            print(f\"Loading GloVe {glove_version} tokens from file...\")\n\n            with open(glove_filepath, encoding=\"utf-8\") as f:\n                lines = f.readlines()\n\n            # Parse GloVe tokens\n            for line in lines:\n                parts = line.strip().split()\n                try:\n                    float(parts[1])\n                except ValueError:\n                    continue\n                else:\n                    token = parts[0].lower()\n                    glove_tokens.append(token)\n\n            initial_size = len(vocab_list)\n            if self.max_vocab_size != -1:\n                for tok in glove_tokens:\n                    if tok not in vocab_list:\n                        vocab_list.append(tok)\n                    if len(vocab_list) == self.max_vocab_size:\n                        break\n            else:\n                vocab_list.extend(glove_tokens)\n                vocab_list = list(set(vocab_list))\n\n            print(f\"Added {len(vocab_list) - initial_size} tokens from GloVe\")\n\n        except Exception as e:\n            print(f\"Error with GloVe processing GloVe: {e}\")\n\n    # Create mappings\n    self.vocab = {token: idx for idx, token in enumerate(vocab_list)}\n    self.vocab_reverse = dict(enumerate(vocab_list))\n\n    del vocab_list\n\n    print(f\"Built vocabulary with {len(self.vocab.items())} tokens.\")\n</code></pre>"},{"location":"modules/data_utils/#src.tfs_mt.data_utils.WordTokenizer.decode","title":"<code>decode(token_ids)</code>","text":"<p>Decode token IDs. Returns the unknown token if the input token is not present in the vocabulary.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids</code> <code>ndarray | list[str]</code> <p>Array or list of tokens ids to decode into text.</p> required <p>Raises:</p> Type Description <code>VocabNotBuiltError</code> <p>Vocabulary is not built.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: Decoded text.</p> Source code in <code>src/tfs_mt/data_utils.py</code> <pre><code>def decode(self, token_ids: np.ndarray | list[str]) -&gt; list[str]:\n    \"\"\"Decode token IDs.\n    Returns the unknown token if the input token is not present in the vocabulary.\n\n    Args:\n        token_ids (np.ndarray | list[str]): Array or list of tokens ids to decode into text.\n\n    Raises:\n        VocabNotBuiltError: Vocabulary is not built.\n\n    Returns:\n        list[str]: Decoded text.\n    \"\"\"\n    if self.vocab_size == 0:\n        raise VocabNotBuiltError()\n    return [self.vocab_reverse.get(idx, self.special_tokens[\"unk_token\"]) for idx in token_ids]\n</code></pre>"},{"location":"modules/data_utils/#src.tfs_mt.data_utils.WordTokenizer.encode","title":"<code>encode(input_sequence, pad_to_len=None, return_only_mask=False)</code>","text":"<p>Tokenizer encode function.</p> <p>It also returns the mask to be used during attention in order not compute it with respect to PAD tokens. The mask is designed to True where there's a token with the model has to compute attention to, False otherwise.</p> <p>Parameters:</p> Name Type Description Default <code>input_sequence</code> <code>str | list[str] | ndarray | Tensor</code> <p>Sequence to be encoded or already encoded (useful in decoding stage when this method is only used to provide the mask).</p> required <code>pad_to_len</code> <code>int | None</code> <p>Sequence length to pad <code>input_sequence</code> with. Defaults to None.</p> <code>None</code> <code>return_only_mask</code> <code>bool</code> <p>Return only attention mask. Defaults to False.</p> <code>False</code> <p>Raises:</p> Type Description <code>VocabNotBuiltError</code> <p>Raised when vocabulary is not built.</p> <p>Returns:</p> Type Description <code>tuple[ndarray, ndarray]</code> <p>tuple[np.ndarray, np.ndarray]: Array of token ids and mask.</p> Source code in <code>src/tfs_mt/data_utils.py</code> <pre><code>def encode(\n    self,\n    input_sequence: str | list[str] | np.ndarray | torch.Tensor,\n    pad_to_len: int | None = None,\n    return_only_mask: bool = False,\n) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Tokenizer encode function.\n\n    It also returns the mask to be used during attention in order not compute it with respect to PAD tokens.\n    The mask is designed to True where there's a token with the model has to compute attention to, False otherwise.\n\n    Args:\n        input_sequence (str | list[str] | np.ndarray | torch.Tensor): Sequence to be encoded or already encoded (useful in decoding stage when this method is only used to provide the mask).\n        pad_to_len (int | None, optional): Sequence length to pad `input_sequence` with. Defaults to None.\n        return_only_mask (bool, optional): Return only attention mask. Defaults to False.\n\n    Raises:\n        VocabNotBuiltError: Raised when vocabulary is not built.\n\n    Returns:\n        tuple[np.ndarray, np.ndarray]: Array of token ids and mask.\n    \"\"\"\n\n    if self.vocab_size == 0:\n        raise VocabNotBuiltError()\n\n    if return_only_mask:\n        if isinstance(input_sequence, np.ndarray):\n            mask = input_sequence != self.pad_token_idx\n        elif isinstance(input_sequence, torch.Tensor):\n            mask = (input_sequence != self.pad_token_idx).to(input_sequence.device)\n        else:\n            raise ValueError()\n        return mask\n\n    # Useful when building a TranslationDataset in order to execute tokenize method only once\n    tokens = self.tokenize(input_sequence) if isinstance(input_sequence, str) else input_sequence\n\n    # Add SOS and EOS tokens to given sequence\n    if tokens[0] != self.special_tokens[\"sos_token\"]:\n        tokens.insert(0, self.special_tokens[\"sos_token\"])\n    if tokens[-1] != self.special_tokens[\"eos_token\"]:\n        tokens.append(self.special_tokens[\"eos_token\"])\n\n    token_ids = [self.vocab.get(token, self.unk_token_idx) for token in tokens]\n\n    if pad_to_len is not None:  # Pad sequence to pad_to_len\n        pad_to_len += 2  # Considering SOS and EOS tokens\n        token_ids.extend([self.pad_token_idx for _ in range(pad_to_len - len(tokens))])\n\n    token_ids = np.array(token_ids, dtype=np.long)\n\n    # Mask to disable attention to pad tokens\n    mask = np.array([token != self.pad_token_idx for token in token_ids], dtype=np.bool)\n\n    return token_ids, mask\n</code></pre>"},{"location":"modules/data_utils/#src.tfs_mt.data_utils.WordTokenizer.tokenize","title":"<code>tokenize(text)</code>","text":"<p>Tokenizer based on GloVe word tokenizer in order to let the model be compatible with GloVe pretrained embeddings.</p> <p>Max word length is 1000. Contractions are treated as distinct tokens, eg. <code>n't</code>, <code>'s</code>, <code>'ll</code>.</p> <p>Reference: GloVe source code</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>text to be tokenized.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: List of string tokens from text.</p> Source code in <code>src/tfs_mt/data_utils.py</code> <pre><code>def tokenize(self, text: str) -&gt; list[str]:\n    \"\"\"Tokenizer based on GloVe word tokenizer in order to let the model be compatible with GloVe pretrained embeddings.\n\n    Max word length is 1000. Contractions are treated as distinct tokens, eg. `n't`, `'s`, `'ll`.\n\n    Reference: [GloVe source code](https://github.com/stanfordnlp/GloVe/blob/master/src/common.c#L75)\n\n    Args:\n        text (str): text to be tokenized.\n\n    Returns:\n        list[str]: List of string tokens from text.\n    \"\"\"\n\n    text = text.strip().lower()\n\n    for contraction in self.contractions:\n        pattern = r\"([a-zA-Z]+)\" + re.escape(contraction) + r\"\\b\"\n        text = re.sub(pattern, r\"\\1 \" + contraction, text)\n\n    for complete_contraction, sub_list in self.nested_contractions_dict.items():\n        pattern = r\"([a-zA-Z]+)\" + re.escape(complete_contraction) + r\"\\b\"\n        if not re.compile(complete_contraction).search(text):\n            for sub_contraction in sub_list:\n                pattern = r\"([a-zA-Z]+)\" + re.escape(sub_contraction) + r\"\\b\"\n                text = re.sub(pattern, r\"\\1 \" + sub_contraction, text)\n        else:\n            text = re.sub(pattern, r\"\\1 \" + complete_contraction, text)\n\n    words = text.split()\n\n    tokens = []\n    for word in words:\n        # Split words when encountering a `'` that's not involved in a quote or a contraction.\n        # eg. \"Quell'ultimo\" gets splitted into \"Quell'\" and \"ultimo\"\n        if (\n            \"'\" in word\n            and word[0] != \"'\"\n            and word[-1] != \"'\"\n            and word not in self.contractions\n            and word not in self.all_sub_contractions\n        ):\n            parts = word.split(\"'\")\n            for i, part in enumerate(parts):\n                if part:\n                    if i &lt; len(parts) - 1:\n                        part += \"'\"\n                    tokens.append(part)\n\n        # Handle trailing punctuation\n        elif word and word[-1] in string.punctuation and word[-1] != \"'\":\n            # Strip all trailing punctuation\n            core_word = word.rstrip(string.punctuation)\n            punct = word[len(core_word) :]\n            if core_word:\n                tokens.append(core_word)\n            for p in punct:\n                tokens.append(p)\n\n        else:\n            if word:\n                tokens.append(word)\n\n    # Truncate sequences by default\n    return tokens[: self.tokenizer_max_len]\n</code></pre>"},{"location":"modules/data_utils/#src.tfs_mt.data_utils.batch_collate_fn","title":"<code>batch_collate_fn(batch, src_pad_token_id, tgt_pad_token_id, pad_all_to_len=-1)</code>","text":"<p>Used to tell the Dataloader how to properly build a batch.</p> <p>In order to correctly build a batch every sequence in it has to have the same length, so it pads the small sequences to the longest one. It does it for <code>src</code>, <code>tgt</code>, <code>src_mask</code> and <code>tgt_mask</code>.</p> <p>This function needs a two pad token ids since in this Trasformer implementation there are 2 distinct tokenizers with their own vocabulary. Each vocabulary is built independently and in parallel, so there's no guarantee that the <code>pad_token</code> will have the same ID in both.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>dict[str, Tensor | list[str]]</code> <p>Batch of token ids and masks.</p> required <code>src_pad_token_id</code> <code>int</code> <p>Pad token for source sequences.</p> required <code>tgt_pad_token_id</code> <code>int</code> <p>Pad token for target sequences.</p> required <code>pad_all_to_len</code> <code>int</code> <p>Sequence length to pad all sequences. If -1 it gets ignored. Defaults to -1.</p> <code>-1</code> <p>Returns:</p> Type Description <code>dict[str, Tensor | list[str]]</code> <p>dict[str, torch.Tensor | list[str]]: Batch with padded sequences.</p> Source code in <code>src/tfs_mt/data_utils.py</code> <pre><code>def batch_collate_fn(\n    batch: dict[str, torch.Tensor | list[str]],\n    src_pad_token_id: int,\n    tgt_pad_token_id: int,\n    pad_all_to_len: int = -1,\n) -&gt; dict[str, torch.Tensor | list[str]]:\n    \"\"\"Used to tell the Dataloader how to properly build a batch.\n\n    In order to correctly build a batch every sequence in it has to have the same length,\n    so it pads the small sequences to the longest one. It does it for `src`, `tgt`, `src_mask` and `tgt_mask`.\n\n    This function needs a two pad token ids since in this Trasformer implementation there are 2 distinct tokenizers\n    with their own vocabulary.\n    Each vocabulary is built independently and in parallel, so there's no guarantee that the `pad_token` will have the same ID in both.\n\n    Args:\n        batch (dict[str, torch.Tensor  |  list[str]]): Batch of token ids and masks.\n        src_pad_token_id (int): Pad token for source sequences.\n        tgt_pad_token_id (int): Pad token for target sequences.\n        pad_all_to_len (int): Sequence length to pad all sequences. If -1 it gets ignored. Defaults to -1.\n\n    Returns:\n        dict[str, torch.Tensor | list[str]]: Batch with padded sequences.\n    \"\"\"\n\n    keys = batch[0].keys()\n    result = {}\n\n    for key in keys:\n        field_data = [sample[key] for sample in batch]\n\n        if isinstance(field_data[0], torch.Tensor):\n            if key == \"src\":\n                pad_token_id = src_pad_token_id\n            elif key == \"tgt\":\n                pad_token_id = tgt_pad_token_id\n            else:  # mask tensors\n                pad_token_id = 0  # will be replaced with False\n\n            if pad_all_to_len != -1:\n                num_padding_values = pad_all_to_len - field_data[0].shape[0]\n                if num_padding_values &gt;= 0:\n                    field_data[0] = F.pad(field_data[0], (0, num_padding_values), value=pad_token_id)\n\n            padded_seq = pad_sequence(field_data, batch_first=True, padding_value=pad_token_id)\n            result[key] = padded_seq\n        else:  # src_text and tgt_text\n            result[key] = field_data\n\n    return result\n</code></pre>"},{"location":"modules/data_utils/#src.tfs_mt.data_utils.build_data_utils","title":"<code>build_data_utils(config, return_all=False, **kwargs)</code>","text":"<p>Build tokenizers, datasets and dataloaders for Machine Translation.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>DictConfig | ListConfig</code> <p>Configuration object from omegaconf.</p> required <code>return_all</code> <code>bool</code> <p>Whether to return dataloaders, datasets and tokenizers. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple[DataLoader, DataLoader] | tuple[DataLoader, DataLoader, TranslationDataset, TranslationDataset, BaseTokenizer, BaseTokenizer]</code> <p>tuple[DataLoader, DataLoader] | tuple[DataLoader, DataLoader, TranslationDataset, TranslationDataset, BaseTokenizer, BaseTokenizer]: Dataloaders or dataloaders, datasets and tokenizers.</p> Source code in <code>src/tfs_mt/data_utils.py</code> <pre><code>def build_data_utils(\n    config: DictConfig | ListConfig, return_all: bool = False, **kwargs\n) -&gt; (\n    tuple[DataLoader, DataLoader]\n    | tuple[DataLoader, DataLoader, TranslationDataset, TranslationDataset, BaseTokenizer, BaseTokenizer]\n):\n    \"\"\"Build tokenizers, datasets and dataloaders for Machine Translation.\n\n    Args:\n        config (DictConfig | ListConfig): Configuration object from omegaconf.\n        return_all (bool, optional): Whether to return dataloaders, datasets and tokenizers. Defaults to False.\n\n    Returns:\n        tuple[DataLoader, DataLoader] | tuple[DataLoader, DataLoader, TranslationDataset, TranslationDataset, BaseTokenizer, BaseTokenizer]: Dataloaders or dataloaders, datasets and tokenizers.\n    \"\"\"\n\n    data = load_dataset(config.dataset.dataset_id, config.dataset.dataset_name, cache_dir=config.cache_ds_path)[\"train\"]\n\n    # TODO If resuming tokenizers from pretrained add check to ensure src and tgt languages matched between the provided config and the checkpoint one\n    src_lang = config.dataset.src_lang\n    tgt_lang = config.dataset.tgt_lang\n\n    # Downsample the dataset. Mainly for computational contraints and to make tests.\n    if config.dataset.max_len != -1:\n        data = data.select(range(config.dataset.max_len))\n\n    print(f\"Train test splitting - {config.dataset.train_split}/{1 - float(config.dataset.train_split):.2f}\")\n    split = data.train_test_split(train_size=config.dataset.train_split, seed=config.seed, shuffle=False)\n    train_data = split[\"train\"]\n    test_data = split[\"test\"]\n\n    try:\n        train_src_texts, train_tgt_texts = [], []\n        for text in train_data[\"translation\"]:\n            train_src_texts.append(text[src_lang])\n            train_tgt_texts.append(text[tgt_lang])\n        test_src_texts, test_tgt_texts = [], []\n        for text in test_data[\"translation\"]:\n            test_src_texts.append(text[src_lang])\n            test_tgt_texts.append(text[tgt_lang])\n    except KeyError as err:  # Mainly here to ensure consistency when resuming model checkpoint\n        raise DatasetLanguagesNotAvailableError(src_lang, tgt_lang) from err\n\n    # Build tokenizers and vocabs. Both src and tgt tokenizers vocabs are built using the training data\n    special_tokens = {\n        \"sos_token\": config.tokenizer.sos_token,\n        \"eos_token\": config.tokenizer.eos_token,\n        \"pad_token\": config.tokenizer.pad_token,\n        \"unk_token\": config.tokenizer.unk_token,\n    }\n\n    # Get tokenizers from pretrained\n    if \"src_tokenizer\" in kwargs and \"tgt_tokenizer\" in kwargs:\n        print(\"Getting tokenizers from pretrained...\")\n        src_tokenizer, tgt_tokenizer = kwargs.get(\"src_tokenizer\"), kwargs.get(\"tgt_tokenizer\")\n\n    else:\n        print(\"Building tokenizers and vocabularies...\")\n        tok_types_dict = {\"word\": WordTokenizer, \"bpe\": BPETokenizer}\n        src_tokenizer = tok_types_dict[config.tokenizer.type](\n            special_tokens,\n            tokenizer_max_len=config.tokenizer.max_seq_len,\n            max_vocab_size=config.tokenizer.max_vocab_size,\n        )\n        tgt_tokenizer = tok_types_dict[config.tokenizer.type](\n            special_tokens,\n            tokenizer_max_len=config.tokenizer.max_seq_len,\n            max_vocab_size=config.tokenizer.max_vocab_size,\n        )\n\n        # To build the vocabularies texts are not filtered based on tokenizer.max_sequence_length\n        src_tokens = [token for text in train_src_texts for token in src_tokenizer.tokenize(text)]\n        tgt_tokens = [token for text in train_tgt_texts for token in tgt_tokenizer.tokenize(text)]\n\n        # BPE Tokenizer\n        if config.tokenizer.type == \"bpe\":\n            # TODO BPE should take all available text in order to extract statistics\n            # and the tokenize method can be done only if the vocabulary is built\n            src_tokenizer.build_vocab(src_tokens)\n            tgt_tokenizer.build_vocab(tgt_tokens)\n        else:\n            src_tokenizer.build_vocab(\n                src_tokens,\n                min_freq=config.tokenizer.vocab_min_freq,\n                extend_with_glove=bool(\n                    src_lang == \"en\" and config.model_configs.pretrained_word_embeddings == \"GloVe\"\n                ),  # GloVe is trained on english only datasets so it doesn't make sense to extend non english vocabs\n                glove_version=config.model_configs[config.chosen_model_size].glove_version,\n            )\n            tgt_tokenizer.build_vocab(\n                tgt_tokens,\n                min_freq=config.tokenizer.vocab_min_freq,\n                extend_with_glove=bool(tgt_lang == \"en\" and config.model_configs.pretrained_word_embeddings == \"GloVe\"),\n                glove_version=config.model_configs[config.chosen_model_size].glove_version,\n            )\n\n    config.tokenizer.src_sos_token_idx = src_tokenizer.sos_token_idx\n    config.tokenizer.src_eos_token_idx = src_tokenizer.eos_token_idx\n    config.tokenizer.src_pad_token_idx = src_tokenizer.pad_token_idx\n    config.tokenizer.src_unk_token_idx = src_tokenizer.unk_token_idx\n    config.tokenizer.tgt_sos_token_idx = tgt_tokenizer.sos_token_idx\n    config.tokenizer.tgt_eos_token_idx = tgt_tokenizer.eos_token_idx\n    config.tokenizer.tgt_pad_token_idx = tgt_tokenizer.pad_token_idx\n    config.tokenizer.tgt_unk_token_idx = tgt_tokenizer.unk_token_idx\n\n    print(\"Building datasets...\")\n    train_dataset = TranslationDataset(\n        train_src_texts,\n        train_tgt_texts,\n        src_tokenizer,\n        tgt_tokenizer,\n        src_lang=config.dataset.src_lang,\n        tgt_lang=config.dataset.tgt_lang,\n        max_sequence_length=config.tokenizer.max_seq_len,\n    )\n    test_dataset = TranslationDataset(\n        test_src_texts,\n        test_tgt_texts,\n        src_tokenizer,\n        tgt_tokenizer,\n        src_lang=config.dataset.src_lang,\n        tgt_lang=config.dataset.tgt_lang,\n        max_sequence_length=config.tokenizer.max_seq_len,\n    )\n\n    print(f\"Train dataset length: {len(train_dataset)}\")\n    print(f\"Test dataset length: {len(test_dataset)}\")\n\n    train_dataloader = DataLoader(\n        train_dataset,\n        batch_size=config.train_dataloader.batch_size,\n        num_workers=config.train_dataloader.num_workers,\n        collate_fn=partial(\n            batch_collate_fn,\n            src_pad_token_id=config.tokenizer.src_pad_token_idx,\n            tgt_pad_token_id=config.tokenizer.tgt_pad_token_idx,\n            pad_all_to_len=config.tokenizer.max_seq_len if config.train_dataloader.pad_all_to_max_len else -1,\n        ),\n        shuffle=config.train_dataloader.shuffle,\n        drop_last=config.train_dataloader.drop_last,\n        persistent_workers=True,  # If True, the dataloader will not shut down the worker processes after a dataset has been consumed once. This allows to maintain the workers Dataset instances alive\n        pin_memory=torch.cuda.is_available(),  # Pinned memory is not used anyway by torch where no accelerator is employed.\n        prefetch_factor=config.train_dataloader.prefetch_factor,\n    )\n    test_dataloader = DataLoader(\n        test_dataset,\n        batch_size=config.test_dataloader.batch_size,\n        num_workers=config.test_dataloader.num_workers,\n        collate_fn=partial(\n            batch_collate_fn,\n            src_pad_token_id=config.tokenizer.src_pad_token_idx,\n            tgt_pad_token_id=config.tokenizer.tgt_pad_token_idx,\n            pad_all_to_len=config.tokenizer.max_seq_len if config.test_dataloader.pad_all_to_max_len else -1,\n        ),\n        shuffle=config.test_dataloader.shuffle,\n        drop_last=config.test_dataloader.drop_last,\n        persistent_workers=True,\n        pin_memory=torch.cuda.is_available(),\n        prefetch_factor=config.train_dataloader.prefetch_factor,\n    )\n\n    print(f\"Train dataloader length: {len(train_dataloader)}\")\n    print(f\"Test dataloader length: {len(test_dataloader)}\")\n\n    if return_all:\n        return train_dataloader, test_dataloader, train_dataset, test_dataset, src_tokenizer, tgt_tokenizer\n    return train_dataloader, test_dataloader\n</code></pre>"},{"location":"modules/data_utils/#src.tfs_mt.data_utils.download_glove","title":"<code>download_glove(output_dir, glove_version='glove.2024.wikigiga.50d')</code>","text":"<p>Download GloVe embeddings and returns the filepath.</p> Source code in <code>src/tfs_mt/data_utils.py</code> <pre><code>def download_glove(output_dir: str, glove_version: str = \"glove.2024.wikigiga.50d\") -&gt; str:\n    \"\"\"Download GloVe embeddings and returns the filepath.\"\"\"\n    glove_folder_path = output_dir + f\"/{glove_version}\"\n    os.makedirs(glove_folder_path, exist_ok=True)\n\n    url = f\"https://nlp.stanford.edu/data/wordvecs/{glove_version}.zip\"\n    zip_path = output_dir + f\"/{glove_version}.zip\"\n\n    glove_filepath = None\n    for file in os.listdir(glove_folder_path):\n        if file.endswith(\".txt\"):\n            glove_filepath = os.path.join(glove_folder_path, file)\n            break\n\n    if glove_filepath is None:\n        print(f\"GloVe not found in {glove_folder_path}. Downloading GloVe ({glove_version})...\")\n\n        response = requests.get(url, stream=True, timeout=600)\n        response.raise_for_status()\n\n        with open(zip_path, \"wb\") as f:\n            for chunk in response.iter_content(chunk_size=8192):\n                f.write(chunk)\n\n        with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n            zip_ref.extractall(glove_folder_path)\n        os.remove(zip_path)\n\n        for file in os.listdir(glove_folder_path):\n            if file.endswith(\".txt\"):\n                glove_filepath = os.path.join(glove_folder_path, file)\n                break\n\n    return glove_filepath\n</code></pre>"},{"location":"modules/data_utils/#src.tfs_mt.data_utils.parse_glove_tokens","title":"<code>parse_glove_tokens(lines)</code>","text":"<p>Parse a chunk of GloVe embeddings file into a list of tokens.</p> <p>For each line it removes any extra spaces, splits it by spaces and take the first part building the tokens' list.</p> <p>It skips \"malformed\" tokens to avoid duplicates in vocabulary. The GloVe file contains tokens with spaces inside, eg. <code>103\u00a03/4</code>, which are not handled by the tokenizer for simplicity.</p> Source code in <code>src/tfs_mt/data_utils.py</code> <pre><code>def parse_glove_tokens(lines: list[str]) -&gt; list[str]:\n    \"\"\"Parse a chunk of GloVe embeddings file into a list of tokens.\n\n    For each line it removes any extra spaces, splits it by spaces and take the first part building the tokens' list.\n\n    It skips \"malformed\" tokens to avoid duplicates in vocabulary.\n    The GloVe file contains tokens with spaces inside, eg. `103\u00a03/4`, which are not handled by the tokenizer for simplicity.\n    \"\"\"\n    result = []\n    for line in lines:\n        parts = line.strip().split()\n        try:\n            float(parts[1])\n        except ValueError:\n            continue\n        else:\n            token = parts[0].lower()\n            result.append(token)\n\n    return result\n</code></pre>"},{"location":"modules/decoding_utils/","title":"Decoding utils","text":""},{"location":"modules/decoding_utils/#src.tfs_mt.decoding_utils.beam_decoding","title":"<code>beam_decoding(model, tgt_tokenizer, src_tokens, src_mask, max_target_tokens=128, output_mode='str')</code>","text":"<p>TBA</p> Source code in <code>src/tfs_mt/decoding_utils.py</code> <pre><code>@torch.inference_mode()\ndef beam_decoding(\n    model: Transformer,\n    tgt_tokenizer: BaseTokenizer,\n    src_tokens: np.ndarray | torch.Tensor,\n    src_mask: np.ndarray | torch.Tensor,\n    max_target_tokens: int = 128,\n    output_mode: str = \"str\",\n) -&gt; list[str]:\n    \"\"\"\n    TBA\n    \"\"\"\n\n    pass\n</code></pre>"},{"location":"modules/decoding_utils/#src.tfs_mt.decoding_utils.greedy_decoding","title":"<code>greedy_decoding(model, tgt_tokenizer, src_tokens, src_mask, max_target_tokens=128, output_mode='str')</code>","text":"<p>Supports batch (decode multiple source sentences) greedy decoding.</p> <p>Decoding could be further optimized to cache old token activations because they can't look ahead and so adding a newly predicted token won't change old token's activations.</p> <p>Example: we input <code>&lt;s&gt;</code> and do a forward pass. We get intermediate activations for <code>&lt;s&gt;</code> and at the output at position 0, after the doing linear layer we get e.g. token <code>&lt;I&gt;</code>. Now we input <code>&lt;s&gt;</code>,<code>&lt;I&gt;</code> but <code>&lt;s&gt;</code>'s activations will remain the same. Similarly say we now got <code>&lt;am&gt;</code> at output position 1, in the next step we input <code>&lt;s&gt;</code>,<code>&lt;I&gt;</code>,<code>&lt;am&gt;</code> and so <code>&lt;I&gt;</code>'s activations will remain the same as it only looks at/attends to itself and to <code>&lt;s&gt;</code> and so forth.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Transformer</code> <p>Encoder-decoder translation model.</p> required <code>tgt_tokenizer</code> <code>BaseTokenizer</code> <p>Target text tokenizer.</p> required <code>src_tokens</code> <code>Tensor</code> <p>Source tokens.</p> required <code>src_mask</code> <code>Tensor</code> <p>Source tokens mask.</p> required <code>max_target_tokens</code> <code>int</code> <p>Max target tokens to output. Defaults to 128.</p> <code>128</code> <code>output_mode</code> <code>str</code> <p>Output mode, if <code>str</code> it will return the sequence as a string, if <code>tokens</code> it will return as list of tokens. Defaults to \"str\".</p> <code>'str'</code> <p>Returns:</p> Type Description <code>list[str] | list[list[str]]</code> <p>list[str] | list[list[str]]: Decoded sequences as strings or as list of tokens.</p> Source code in <code>src/tfs_mt/decoding_utils.py</code> <pre><code>@torch.inference_mode()\ndef greedy_decoding(\n    model: Transformer,\n    tgt_tokenizer: BaseTokenizer,\n    src_tokens: np.ndarray | torch.Tensor,\n    src_mask: np.ndarray | torch.Tensor,\n    max_target_tokens: int = 128,\n    output_mode: str = \"str\",\n) -&gt; list[str] | list[list[str]]:\n    \"\"\"\n    Supports batch (decode multiple source sentences) greedy decoding.\n\n    Decoding could be further optimized to cache old token activations because they can't look ahead and so\n    adding a newly predicted token won't change old token's activations.\n\n    Example: we input `&lt;s&gt;` and do a forward pass. We get intermediate activations for `&lt;s&gt;` and at the output at position\n    0, after the doing linear layer we get e.g. token `&lt;I&gt;`. Now we input `&lt;s&gt;`,`&lt;I&gt;` but `&lt;s&gt;`'s activations will remain\n    the same. Similarly say we now got `&lt;am&gt;` at output position 1, in the next step we input `&lt;s&gt;`,`&lt;I&gt;`,`&lt;am&gt;` and so `&lt;I&gt;`'s\n    activations will remain the same as it only looks at/attends to itself and to `&lt;s&gt;` and so forth.\n\n    Args:\n        model (Transformer): Encoder-decoder translation model.\n        tgt_tokenizer (BaseTokenizer): Target text tokenizer.\n        src_tokens (torch.Tensor): Source tokens.\n        src_mask (torch.Tensor): Source tokens mask.\n        max_target_tokens (int, optional): Max target tokens to output. Defaults to 128.\n        output_mode (str, optional): Output mode, if `str` it will return the sequence as a string, if `tokens` it will return as list of tokens. Defaults to \"str\".\n\n    Returns:\n        list[str] | list[list[str]]: Decoded sequences as strings or as list of tokens.\n    \"\"\"\n\n    device = next(model.parameters()).device\n\n    sos_token = tgt_tokenizer.decode([tgt_tokenizer.sos_token_idx])[0]\n    eos_token = tgt_tokenizer.decode([tgt_tokenizer.eos_token_idx])[0]\n\n    # Get encoder representation\n    if isinstance(src_tokens, np.ndarray) and isinstance(src_tokens, np.ndarray):\n        src_tokens = torch.tensor(src_tokens)\n        src_mask = torch.tensor(src_mask)\n    if src_tokens.ndim == 1 and src_mask.ndim == 1:\n        src_tokens.unsqueeze_(0)\n        src_mask.unsqueeze_(0)\n\n    src_tokens = src_tokens.to(device)\n    src_mask = src_mask.to(device)\n\n    encoder_representation = model.encode(src_tokens, src_mask)\n\n    # Generate a batch of sequences starting with SOS token, batch size is inferred by the encoder representation tensor\n    tgt_sequence_batch_text = [[sos_token] for _ in range(encoder_representation.shape[0])]\n    tgt_sequence_batch = torch.tensor(\n        [[tgt_tokenizer.sos_token_idx] for _ in range(encoder_representation.shape[0])], device=device\n    )\n\n    # This list handles when to stop the tokens generation for each sequence in the batch\n    is_decoded = [False] * encoder_representation.shape[0]\n\n    while True:\n        tgt_mask = tgt_tokenizer.encode(tgt_sequence_batch, return_only_mask=True)\n\n        # Due to cross attention max tgt sequences cannot be longer than max src sequences\n        if tgt_sequence_batch.shape[1] &gt; encoder_representation.shape[1]:\n            dummy_tensor = torch.ones_like(encoder_representation, device=encoder_representation.device)\n            dummy_tensor = dummy_tensor[:, 0, :].unsqueeze(1)\n            encoder_representation = torch.cat((encoder_representation, dummy_tensor), dim=1)\n\n            addon_mask = torch.zeros((src_mask.shape[0], 1), dtype=torch.bool, device=src_mask.device)\n            src_mask = torch.cat((src_mask, addon_mask), dim=1)\n\n        # Shape = (B*T, V) where T is the current token-sequence length and V target vocab size\n        decoder_output = model.decode(tgt_sequence_batch, encoder_representation, tgt_mask, src_mask)\n\n        # Extract only the indices of last token for every target sentence\n        num_of_tgt_tokens = tgt_sequence_batch.shape[1]\n        decoder_output = decoder_output[:, num_of_tgt_tokens - 1 :: num_of_tgt_tokens]\n\n        # Greedy decode tokens selecting the most probable one and discard other tokens\n        most_probable_last_token_indices = torch.argmax(decoder_output, dim=-1).cpu().numpy()\n\n        # Find target tokens associated with these indices\n        predicted_words = []\n        for row in most_probable_last_token_indices:\n            predicted_words.append(tgt_tokenizer.decode(row)[0])\n\n        for idx, predicted_word in enumerate(predicted_words):\n            tgt_sequence_batch_text[idx].append(predicted_word)\n            # Once EOS token is generated for a sentence in the batch it gets flagged in is_decoded list\n            if predicted_word == eos_token:\n                is_decoded[idx] = True\n\n        if all(is_decoded) or num_of_tgt_tokens == max_target_tokens:\n            break\n\n        # Prepare the input for the next iteration: merge old token ids with the new column of most probable token ids\n        tgt_sequence_batch = torch.cat(\n            (tgt_sequence_batch, torch.tensor(most_probable_last_token_indices, device=device)), dim=1\n        )\n\n    # Post process the sentences: remove everything after the EOS token\n    post_processed_sequences = []\n    for tgt_sequence in tgt_sequence_batch_text:\n        try:\n            target_index = tgt_sequence.index(eos_token) + 1\n        except ValueError:\n            target_index = None\n\n        tgt_sequence = tgt_sequence[:target_index]\n        post_processed_sequences.append(tgt_sequence)\n\n    if output_mode == \"str\":\n        post_processed_sequences_str = []\n        for i in range(len(post_processed_sequences)):\n            seq = post_processed_sequences[i]\n            seq.remove(sos_token)\n            if eos_token in seq:\n                seq.remove(eos_token)\n            sequence_as_str = \" \".join(seq)\n            post_processed_sequences_str.append(sequence_as_str)\n        post_processed_sequences = post_processed_sequences_str\n\n    return post_processed_sequences\n</code></pre>"},{"location":"modules/embeddings/","title":"Embeddings","text":""},{"location":"modules/embeddings/#src.tfs_mt.embeddings.Embedding","title":"<code>Embedding</code>","text":"<p>               Bases: <code>Module</code></p> <p>Transformer embeddings layer.</p> <p>This implementation uses a randomly initialized embedding lookup table with dimension <code>[vocab_size, d_model]</code>.</p> <p>There's the possibility of loading pretrained embeddings from GloVe. This choice has been made to achieve acceptable performances with low resources training and limited time training.</p> <p>Parameters:</p> Name Type Description Default <code>vocab_size</code> <code>int</code> <p>Number of tokens in vocabulary.</p> required <code>d_model</code> <code>int | None</code> <p>Model dimension. Defaults to None.</p> <code>None</code> <code>from_pretrained</code> <code>bool</code> <p>Load embeddings from pretrained. Defaults to False.</p> <code>False</code> <code>pretrained_emb_type</code> <code>str | None</code> <p>Type of pretrained embeddings. Defaults to None.</p> <code>None</code> <code>pretrained_emb_path</code> <code>str | None</code> <p>Path of pretrained embeddings. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>EmbeddingDimError</code> <p>Raised when the provided embedding dimension does not match the expected size.</p> <code>EmbeddingTypePathError</code> <p>Raised when the embedding type or file path is invalid or not found.</p> <code>TokenizerNotSuppliedError</code> <p>Raised when a tokenizer is required but has not been supplied.</p> <code>VocabNotBuiltError</code> <p>Raised when an operation requiring a built vocabulary is attempted before the vocabulary is constructed.</p> Source code in <code>src/tfs_mt/embeddings.py</code> <pre><code>class Embedding(nn.Module):\n    \"\"\"Transformer embeddings layer.\n\n    This implementation uses a randomly initialized embedding lookup table with dimension `[vocab_size, d_model]`.\n\n    There's the possibility of loading pretrained embeddings from GloVe. This choice has been made to achieve acceptable\n    performances with low resources training and limited time training.\n\n    Args:\n        vocab_size (int): Number of tokens in vocabulary.\n        d_model (int | None, optional): Model dimension. Defaults to None.\n        from_pretrained (bool, optional): Load embeddings from pretrained. Defaults to False.\n        pretrained_emb_type (str | None, optional): Type of pretrained embeddings. Defaults to None.\n        pretrained_emb_path (str | None, optional): Path of pretrained embeddings. Defaults to None.\n\n    Raises:\n        EmbeddingDimError: Raised when the provided embedding dimension does not match the expected size.\n        EmbeddingTypePathError: Raised when the embedding type or file path is invalid or not found.\n        TokenizerNotSuppliedError: Raised when a tokenizer is required but has not been supplied.\n        VocabNotBuiltError: Raised when an operation requiring a built vocabulary is attempted before the vocabulary is constructed.\n    \"\"\"\n\n    def __init__(\n        self,\n        vocab_size: int,\n        d_model: int | None = None,\n        from_pretrained: bool = False,\n        pretrained_emb_type: str | None = None,\n        pretrained_emb_path: str | None = None,\n        **kwargs,\n    ):\n        super().__init__()\n\n        if d_model is None and not from_pretrained:\n            raise EmbeddingDimError(d_model, from_pretrained)\n\n        if from_pretrained:\n            if d_model is not None:\n                print(f\"Ignoring d_model ({d_model}). The embeddings dim will be inferred from pretrained.\")\n            if pretrained_emb_type is None or pretrained_emb_path is None:\n                raise EmbeddingTypePathError(from_pretrained, pretrained_emb_type, pretrained_emb_path)\n            if pretrained_emb_type == \"GloVe\" and \"tokenizer\" not in kwargs:\n                raise TokenizerNotSuppliedError()\n            if kwargs[\"tokenizer\"].vocab_size == 0:\n                raise VocabNotBuiltError()\n\n            if pretrained_emb_type == \"GloVe\":\n                embeddings_dim, embeddings_lut = self.load_pretrained(\n                    pretrained_emb_path, pretrained_emb_type, tokenizer=kwargs[\"tokenizer\"]\n                )\n            else:\n                embeddings_dim, embeddings_lut = self.load_pretrained(pretrained_emb_path, pretrained_emb_type)\n\n            # The following operations consist in rescaling the norm of GloVe pretrained embeddings.\n            # This is done to have source and target embeddings with a comparable norm to stabilize training.\n            embeddings_lut_dummy = nn.Embedding(embeddings_lut.weight.shape[0], embeddings_lut.weight.shape[1])\n            nn.init.xavier_uniform_(embeddings_lut_dummy.weight)\n\n            embeddings_lut_dummy_norm = torch.mean(embeddings_lut_dummy.weight.data.norm(dim=1))\n            embeddings_lut_norm = torch.mean(embeddings_lut.weight.data.norm(dim=1))\n\n            rescale_coeff = embeddings_lut_dummy_norm / embeddings_lut_norm\n\n            embeddings_lut.weight.data = embeddings_lut.weight.data * rescale_coeff\n\n        else:\n            embeddings_dim = d_model\n            # Randomly initialized lookup table.\n            embeddings_lut = nn.Embedding(vocab_size, embeddings_dim)\n            nn.init.xavier_uniform_(embeddings_lut.weight)\n\n        self.d_model = embeddings_dim\n        self.scaling_factor = math.sqrt(self.d_model)\n        self.embeddings_lut = embeddings_lut\n\n    def load_pretrained(self, embeddings_path: str, emb_type: str = \"GloVe\", **kwargs) -&gt; tuple[int, nn.Embedding]:\n        \"\"\"Loads pretrained GloVe embedding into the embedding lookup table.\"\"\"\n        if emb_type == \"GloVe\":\n            tokenizer = kwargs[\"tokenizer\"]\n\n            if not os.path.isfile(embeddings_path):\n                output_dir = \"/\".join(embeddings_path.split(\"/\")[:-2])\n                download_glove(output_dir, glove_version=embeddings_path.split(\"/\")[-2])\n\n            with open(embeddings_path, encoding=\"utf-8\") as f:\n                embeddings_dim = len(f.readline().strip().split()) - 1\n            embeddings_lut = nn.Embedding(tokenizer.vocab_size, embeddings_dim)\n\n            # NOTE The vocab extension with GloVe tokens is handled by the tokenizer.\n            # Here GloVe token embeddings are mapped to the corresponding entry in the embeddings lookup table\n            with open(embeddings_path, encoding=\"utf-8\") as f:\n                for line in f:\n                    parts = line.strip().split()\n                    idx, _ = tokenizer.encode(parts[0])\n                    # If tokenization if non perfectly compatible with the GloVe one, idx can be a sequence of tokens longer then 3\n                    # (Consider the first and last tokens coming out of tokenizer.encode are SOS_TOKEN and EOS_TOKEN)\n                    # This is a useful check to avoid overwriting in the embeddings_lut matrix\n                    if len(idx) &gt; 3:\n                        continue\n                    idx = idx[1]  # The first token coming out of tokenizer.encode is SOS_TOKEN\n                    if len(parts[1:]) != embeddings_dim:  # Skip unhandled tokens with spaces, eg. \"1 3/4\"\n                        continue\n                    try:\n                        token_emb = torch.tensor([float(x) for x in parts[1:]], dtype=torch.float32)\n                    except ValueError:\n                        continue\n                    else:\n                        embeddings_lut.weight.data[idx].copy_(token_emb)\n        else:\n            raise EmbeddingTypeNotImplementedError(emb_type)\n\n        return embeddings_dim, embeddings_lut\n\n    def forward(self, token_ids: Float[torch.Tensor, \"B S\"]) -&gt; Float[torch.Tensor, \"B S D\"]:\n        \"\"\"Get token embeddings.\n\n        Args:\n            token_ids (Float[torch.Tensor, \"B S\"]): Input batch of token_ids. Where B is the batch size and S is the sequence length.\n\n        Returns:\n            Float[torch.Tensor, \"B S D\"]: Output batch of token embeddings. Where D is d_model.\n        \"\"\"\n\n        embeddings = self.embeddings_lut(token_ids)\n\n        # In the embedding layers, we multiply those weights by sqrt(d_model) (Attention is all you need page 5)\n        embeddings = embeddings * self.scaling_factor\n\n        return embeddings\n</code></pre>"},{"location":"modules/embeddings/#src.tfs_mt.embeddings.Embedding.forward","title":"<code>forward(token_ids)</code>","text":"<p>Get token embeddings.</p> <p>Parameters:</p> Name Type Description Default <code>token_ids</code> <code>Float[Tensor, 'B S']</code> <p>Input batch of token_ids. Where B is the batch size and S is the sequence length.</p> required <p>Returns:</p> Type Description <code>Float[Tensor, 'B S D']</code> <p>Float[torch.Tensor, \"B S D\"]: Output batch of token embeddings. Where D is d_model.</p> Source code in <code>src/tfs_mt/embeddings.py</code> <pre><code>def forward(self, token_ids: Float[torch.Tensor, \"B S\"]) -&gt; Float[torch.Tensor, \"B S D\"]:\n    \"\"\"Get token embeddings.\n\n    Args:\n        token_ids (Float[torch.Tensor, \"B S\"]): Input batch of token_ids. Where B is the batch size and S is the sequence length.\n\n    Returns:\n        Float[torch.Tensor, \"B S D\"]: Output batch of token embeddings. Where D is d_model.\n    \"\"\"\n\n    embeddings = self.embeddings_lut(token_ids)\n\n    # In the embedding layers, we multiply those weights by sqrt(d_model) (Attention is all you need page 5)\n    embeddings = embeddings * self.scaling_factor\n\n    return embeddings\n</code></pre>"},{"location":"modules/embeddings/#src.tfs_mt.embeddings.Embedding.load_pretrained","title":"<code>load_pretrained(embeddings_path, emb_type='GloVe', **kwargs)</code>","text":"<p>Loads pretrained GloVe embedding into the embedding lookup table.</p> Source code in <code>src/tfs_mt/embeddings.py</code> <pre><code>def load_pretrained(self, embeddings_path: str, emb_type: str = \"GloVe\", **kwargs) -&gt; tuple[int, nn.Embedding]:\n    \"\"\"Loads pretrained GloVe embedding into the embedding lookup table.\"\"\"\n    if emb_type == \"GloVe\":\n        tokenizer = kwargs[\"tokenizer\"]\n\n        if not os.path.isfile(embeddings_path):\n            output_dir = \"/\".join(embeddings_path.split(\"/\")[:-2])\n            download_glove(output_dir, glove_version=embeddings_path.split(\"/\")[-2])\n\n        with open(embeddings_path, encoding=\"utf-8\") as f:\n            embeddings_dim = len(f.readline().strip().split()) - 1\n        embeddings_lut = nn.Embedding(tokenizer.vocab_size, embeddings_dim)\n\n        # NOTE The vocab extension with GloVe tokens is handled by the tokenizer.\n        # Here GloVe token embeddings are mapped to the corresponding entry in the embeddings lookup table\n        with open(embeddings_path, encoding=\"utf-8\") as f:\n            for line in f:\n                parts = line.strip().split()\n                idx, _ = tokenizer.encode(parts[0])\n                # If tokenization if non perfectly compatible with the GloVe one, idx can be a sequence of tokens longer then 3\n                # (Consider the first and last tokens coming out of tokenizer.encode are SOS_TOKEN and EOS_TOKEN)\n                # This is a useful check to avoid overwriting in the embeddings_lut matrix\n                if len(idx) &gt; 3:\n                    continue\n                idx = idx[1]  # The first token coming out of tokenizer.encode is SOS_TOKEN\n                if len(parts[1:]) != embeddings_dim:  # Skip unhandled tokens with spaces, eg. \"1 3/4\"\n                    continue\n                try:\n                    token_emb = torch.tensor([float(x) for x in parts[1:]], dtype=torch.float32)\n                except ValueError:\n                    continue\n                else:\n                    embeddings_lut.weight.data[idx].copy_(token_emb)\n    else:\n        raise EmbeddingTypeNotImplementedError(emb_type)\n\n    return embeddings_dim, embeddings_lut\n</code></pre>"},{"location":"modules/embeddings/#src.tfs_mt.embeddings.SinusoidalPositionalEncoding","title":"<code>SinusoidalPositionalEncoding</code>","text":"<p>               Bases: <code>Module</code></p> <p>Sinusoidal Positional Encoding implementation from the original paper.</p> \\[ \\begin{align*} PE_{(pos,2i)} &amp;= \\sin(pos/10000^{2i/d_{model}}) \\\\ PE_{(pos,2i+1)} &amp;= \\cos(pos/10000^{2i/d_{model}}) \\end{align*} \\] <p>Parameters:</p> Name Type Description Default <code>d_model</code> <code>int</code> <p>Model dimension.</p> required <code>dropout_prob</code> <code>float</code> <p>Dropout probability. Defaults to 0.1.</p> <code>0.1</code> <code>max_sequence_length</code> <code>int</code> <p>Max sequence length. Defaults to 128.</p> <code>128</code> Source code in <code>src/tfs_mt/embeddings.py</code> <pre><code>class SinusoidalPositionalEncoding(nn.Module):\n    \"\"\"Sinusoidal Positional Encoding implementation from the [original paper](https://arxiv.org/abs/1706.03762).\n\n    $$\n    \\\\begin{align*}\n    PE_{(pos,2i)} &amp;= \\\\sin(pos/10000^{2i/d_{model}}) \\\\\\\\\n    PE_{(pos,2i+1)} &amp;= \\\\cos(pos/10000^{2i/d_{model}})\n    \\\\end{align*}\n    $$\n\n    Args:\n        d_model (int): Model dimension.\n        dropout_prob (float, optional): Dropout probability. Defaults to 0.1.\n        max_sequence_length (int, optional): Max sequence length. Defaults to 128.\n    \"\"\"\n\n    def __init__(self, d_model: int, dropout_prob: float = 0.1, max_sequence_length: int = 128):\n        super().__init__()\n\n        # Vector of all possible positions in the sequence [max_sequence_length, 1]\n        position_id = torch.arange(0, max_sequence_length).unsqueeze(1)\n        i_idx = torch.arange(0, d_model, 2, dtype=torch.float32) / d_model  # 2i / d_model\n        freq_vec = torch.pow(10000.0, -i_idx)  # 1 / (10000^(2i/d_model))\n\n        pe_lut = torch.zeros(max_sequence_length, d_model)  # Init positional encoding lookup table\n        pe_lut[:, 0::2] = torch.sin(position_id * freq_vec)  # Assign sine on even positions\n        pe_lut[:, 1::2] = torch.cos(position_id * freq_vec)  # Assing cosine on odd positions\n\n        # Registering this weights as buffers so that they will be saved in model state_dict,\n        # but they won't appear in model.parameters so that optimizer will not change them\n        self.register_buffer(\"pe_lut\", pe_lut)\n\n        self.dropout = nn.Dropout(dropout_prob)\n\n    def forward(self, token_embeddings: Float[torch.Tensor, \"B S D\"]) -&gt; Float[torch.Tensor, \"B S D\"]:\n        \"\"\"Get token embeddings with positional information.\n\n        Args:\n            token_embeddings (Float[torch.Tensor, \"B S D\"]): Input batch of token embeddings.\n\n        Raises:\n            IncompatibleEmbeddingsDimError: Raised when the input embedding dimension is invalid.\n\n        Returns:\n            Float[torch.Tensor, \"B S D\"]: Token embeddings with added positional information.\n        \"\"\"\n        if token_embeddings.ndim != 3 or token_embeddings.size(-1) != self.pe_lut.shape[1]:\n            raise IncompatibleEmbeddingsDimError(token_embeddings.shape)\n\n        positional_encodings = self.pe_lut[: token_embeddings.size(1)]\n\n        # we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks (Attention is all you need page 8)\n        final_embedding = self.dropout(token_embeddings + positional_encodings)\n\n        return final_embedding\n</code></pre>"},{"location":"modules/embeddings/#src.tfs_mt.embeddings.SinusoidalPositionalEncoding.forward","title":"<code>forward(token_embeddings)</code>","text":"<p>Get token embeddings with positional information.</p> <p>Parameters:</p> Name Type Description Default <code>token_embeddings</code> <code>Float[Tensor, 'B S D']</code> <p>Input batch of token embeddings.</p> required <p>Raises:</p> Type Description <code>IncompatibleEmbeddingsDimError</code> <p>Raised when the input embedding dimension is invalid.</p> <p>Returns:</p> Type Description <code>Float[Tensor, 'B S D']</code> <p>Float[torch.Tensor, \"B S D\"]: Token embeddings with added positional information.</p> Source code in <code>src/tfs_mt/embeddings.py</code> <pre><code>def forward(self, token_embeddings: Float[torch.Tensor, \"B S D\"]) -&gt; Float[torch.Tensor, \"B S D\"]:\n    \"\"\"Get token embeddings with positional information.\n\n    Args:\n        token_embeddings (Float[torch.Tensor, \"B S D\"]): Input batch of token embeddings.\n\n    Raises:\n        IncompatibleEmbeddingsDimError: Raised when the input embedding dimension is invalid.\n\n    Returns:\n        Float[torch.Tensor, \"B S D\"]: Token embeddings with added positional information.\n    \"\"\"\n    if token_embeddings.ndim != 3 or token_embeddings.size(-1) != self.pe_lut.shape[1]:\n        raise IncompatibleEmbeddingsDimError(token_embeddings.shape)\n\n    positional_encodings = self.pe_lut[: token_embeddings.size(1)]\n\n    # we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks (Attention is all you need page 8)\n    final_embedding = self.dropout(token_embeddings + positional_encodings)\n\n    return final_embedding\n</code></pre>"},{"location":"modules/training_utils/","title":"Training utils","text":""},{"location":"modules/training_utils/#src.tfs_mt.training_utils.get_param_groups","title":"<code>get_param_groups(model_named_parameters, weight_decay)</code>","text":"<p>Create parameter groups for optimizer with selective weight decay application.</p> <p>Parameters excluded from weight decay: - Bias terms - Layer normalization weights - Embedding weights</p> <p>Parameters:</p> Name Type Description Default <code>model_named_parameters</code> <code>Iterable[tuple[str, Parameter]]</code> <p>Model's named parameters.</p> required <code>weight_decay</code> <code>float</code> <p>Weight decay coefficient to apply to parameters as mentioned.</p> required <p>Returns:</p> Type Description <code>list[dict[str, Any]]</code> <p>list[dict[str, Any]]: List of two parameter group dictionaries to be passed to the optimizer.</p> Source code in <code>src/tfs_mt/training_utils.py</code> <pre><code>def get_param_groups(\n    model_named_parameters: Iterable[tuple[str, nn.Parameter]], weight_decay: float\n) -&gt; list[dict[str, Any]]:\n    \"\"\"Create parameter groups for optimizer with selective weight decay application.\n\n    Parameters excluded from weight decay:\n    - Bias terms\n    - Layer normalization weights\n    - Embedding weights\n\n    Args:\n        model_named_parameters (Iterable[tuple[str, nn.Parameter]]): Model's named parameters.\n        weight_decay (float): Weight decay coefficient to apply to parameters as mentioned.\n\n    Returns:\n        list[dict[str, Any]]: List of two parameter group dictionaries to be passed to the optimizer.\n    \"\"\"\n\n    decay = []\n    no_decay = []\n    for name, param in model_named_parameters:\n        if not param.requires_grad:\n            continue\n        # Exclude biases, LayerNorm and embeddings from weight decay regularization\n        if name.endswith(\".bias\") or \"layer_norm\" in name or \"embeddings\" in name:\n            no_decay.append(param)\n        else:\n            decay.append(param)\n\n    return [\n        {\"params\": decay, \"weight_decay\": weight_decay},\n        {\"params\": no_decay, \"weight_decay\": 0.0},\n    ]\n</code></pre>"},{"location":"modules/training_utils/#src.tfs_mt.training_utils.log_metrics","title":"<code>log_metrics(engine, tag)</code>","text":"<p>Log <code>engine.state.metrics</code> with given <code>engine</code> and <code>tag</code>.</p> <p>Parameters:</p> Name Type Description Default <code>engine</code> <code>Engine</code> <p>Instance of <code>Engine</code> which metrics to log.</p> required <code>tag</code> <code>str</code> <p>A string to add at the start of output.</p> required Source code in <code>src/tfs_mt/training_utils.py</code> <pre><code>def log_metrics(engine: Engine, tag: str) -&gt; None:\n    \"\"\"Log `engine.state.metrics` with given `engine` and `tag`.\n\n    Args:\n        engine (Engine): Instance of `Engine` which metrics to log.\n        tag (str): A string to add at the start of output.\n    \"\"\"\n\n    metrics_format = f\"{tag} [{engine.state.epoch}/{engine.state.iteration}]: {engine.state.metrics}\"\n    engine.logger.info(metrics_format)\n</code></pre>"},{"location":"modules/training_utils/#src.tfs_mt.training_utils.loss_metric_transform","title":"<code>loss_metric_transform(output, loss_type)</code>","text":"<p>Transform <code>eval_one_iter</code> output to be compliant with torch loss computation.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>tuple[Tensor, Tensor]</code> <p>Output of <code>eval_one_iter</code>.</p> required <code>loss_type</code> <code>str</code> <p>Loss type choosen.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor]</code> <p>tuple[torch.Tensor, torch.Tensor]: Loss compatible output.</p> Source code in <code>src/tfs_mt/training_utils.py</code> <pre><code>def loss_metric_transform(\n    output: tuple[torch.Tensor, torch.Tensor],\n    loss_type: str,\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Transform `eval_one_iter` output to be compliant with torch loss computation.\n\n    Args:\n        output (tuple[torch.Tensor, torch.Tensor]): Output of `eval_one_iter`.\n        loss_type (str): Loss type choosen.\n\n    Returns:\n        tuple[torch.Tensor, torch.Tensor]: Loss compatible output.\n    \"\"\"\n\n    output_logits, tgt_output_label = output\n\n    if loss_type == \"crossentropy\":\n        pred = output_logits\n    elif loss_type == \"KLdiv-labelsmoothing\":\n        pred = F.log_softmax(output_logits, dim=-1)\n    else:\n        raise ValueError(f\"Invalid loss type, got {loss_type}\")\n\n    return (pred.reshape(-1, pred.size(-1)), tgt_output_label.reshape(-1))\n</code></pre>"},{"location":"modules/training_utils/#src.tfs_mt.training_utils.nlp_metric_transform","title":"<code>nlp_metric_transform(output, tgt_tokenizer)</code>","text":"<p>Transform <code>eval_one_iter</code> output to be compliant with ignite nlp metrics.</p> <p>References: 1. Bleu documentation page [link] 2. Rouge documentation page [link]</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>tuple[Tensor, Tensor]</code> <p>Output of <code>eval_one_iter</code>.</p> required <code>tgt_tokenizer</code> <code>BaseTokenizer</code> <p>Target tokenizer used to decode tokens.</p> required <p>Returns:</p> Type Description <code>tuple[list[list[str]], list[list[list[str]]]]</code> <p>tuple[list[list[str]], list[list[list[str]]]]: Metrics complatible output.</p> Source code in <code>src/tfs_mt/training_utils.py</code> <pre><code>def nlp_metric_transform(\n    output: tuple[torch.Tensor, torch.Tensor], tgt_tokenizer: BaseTokenizer\n) -&gt; tuple[list[list[str]], list[list[list[str]]]]:\n    \"\"\"Transform `eval_one_iter` output to be compliant with ignite nlp metrics.\n\n    References:\n    1. Bleu documentation page [[link](https://docs.pytorch.org/ignite/generated/ignite.metrics.Bleu.html)]\n    2. Rouge documentation page [[link](https://docs.pytorch.org/ignite/generated/ignite.metrics.Rouge.html)]\n\n    Args:\n        output (tuple[torch.Tensor, torch.Tensor]): Output of `eval_one_iter`.\n        tgt_tokenizer (BaseTokenizer): Target tokenizer used to decode tokens.\n\n    Returns:\n        tuple[list[list[str]], list[list[list[str]]]]: Metrics complatible output.\n    \"\"\"\n\n    output_logits, tgt_output_label = output\n\n    # Get predicted tokens from logits\n    output_logits = output_logits.detach()\n    output_tokens = torch.argmax(output_logits, dim=-1)\n\n    # Move to list of int for tokenizer.decode compatibility\n    output_tokens = output_tokens.cpu().numpy().tolist()\n    tgt_output_label = tgt_output_label.cpu().numpy().tolist()\n\n    # Decode batched token sequences to lists of lists of vocab token sequences\n    y_pred = [tgt_tokenizer.decode(sample) for sample in output_tokens]\n    y = [tgt_tokenizer.decode(sample) for sample in tgt_output_label]\n\n    # Adjust shape. Ignite wants a corpus of lists of target label sentences for each hypotheses.\n    # Since the dataset proposes only one target translation for a given input, y is wrapped in a list.\n    y = [y]\n\n    return y_pred, y\n</code></pre>"},{"location":"modules/training_utils/#src.tfs_mt.training_utils.resume_from_ckpt","title":"<code>resume_from_ckpt(checkpoint_path, to_load=None, device=None, logger=None, strict=True, resume_tokenizers=False, tokenizers_type='word')</code>","text":"<p>Loads state dict from a checkpoint file to resume the training or loads tokenizers. It supports loading from local or bucket s3 checkpoint.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint_path</code> <code>str</code> <p>Path to the checkpoint file (or folder) in S3 bucket or in filesystem.</p> required <code>to_load</code> <code>Mapping | None</code> <p>A dictionary with objects.. Defaults to None.</p> <code>None</code> <code>device</code> <code>device | None</code> <p>Device. Defaults to None.</p> <code>None</code> <code>logger</code> <code>Logger | None</code> <p>To log info about resuming from a checkpoint. Defaults to None.</p> <code>None</code> <code>strict</code> <code>bool</code> <p>Whether to strictly enforce that the keys in <code>state_dict</code> match the keys returned by this module's <code>state_dict()</code> function. Defaults to True.</p> <code>True</code> <code>resume_tokenizers</code> <code>bool</code> <p>Whether to load only tokenizers. Defaults to False.</p> <code>False</code> <code>tokenizers_type</code> <code>str</code> <p>Tokenizers type (Word, BPE).</p> <code>'word'</code> <p>Raises:</p> Type Description <code>CheckpointNotFoundError</code> <p>Raised when checkpoint file doesn't exist.</p> <code>InvalidCheckpointS3PathError</code> <p>Raised when bucket and file key are not correctly extracted from provided url.</p> <code>S3FailedDownloadError</code> <p>Raised when download fails.</p> <p>Returns:</p> Type Description <code>None | tuple[BaseTokenizer, BaseTokenizer]</code> <p>None | tuple[BaseTokenizer, BaseTokenizer]: Pretrained tokenizers if resume_tokenizers. Otherwise None.</p> Source code in <code>src/tfs_mt/training_utils.py</code> <pre><code>def resume_from_ckpt(\n    checkpoint_path: str,\n    to_load: Mapping | None = None,\n    device: torch.device | None = None,\n    logger: Logger | None = None,\n    strict: bool = True,\n    resume_tokenizers: bool = False,\n    tokenizers_type: str | None = \"word\",\n) -&gt; None | tuple[BaseTokenizer, BaseTokenizer]:\n    \"\"\"Loads state dict from a checkpoint file to resume the training or loads tokenizers.\n    It supports loading from local or bucket s3 checkpoint.\n\n    Args:\n        checkpoint_path (str): Path to the checkpoint file (or folder) in S3 bucket or in filesystem.\n        to_load (Mapping | None, optional): A dictionary with objects.. Defaults to None.\n        device (torch.device | None, optional): Device. Defaults to None.\n        logger (Logger | None, optional): To log info about resuming from a checkpoint. Defaults to None.\n        strict (bool, optional): Whether to strictly enforce that the keys in `state_dict` match the keys returned by this module's `state_dict()` function. Defaults to True.\n        resume_tokenizers (bool, optional): Whether to load only tokenizers. Defaults to False.\n        tokenizers_type (str, optional): Tokenizers type (Word, BPE).\n\n    Raises:\n        CheckpointNotFoundError: Raised when checkpoint file doesn't exist.\n        InvalidCheckpointS3PathError: Raised when bucket and file key are not correctly extracted from provided url.\n        S3FailedDownloadError: Raised when download fails.\n\n    Returns:\n        None | tuple[BaseTokenizer, BaseTokenizer]: Pretrained tokenizers if resume_tokenizers. Otherwise None.\n    \"\"\"\n\n    resume_method = \"local\"\n    if checkpoint_path.startswith(\"s3://\"):\n        resume_method = \"bucket-s3\"\n\n    if resume_method == \"local\":\n        if not os.path.isfile(checkpoint_path):\n            raise CheckpointNotFoundError(checkpoint_path)\n\n        if resume_tokenizers:\n            # Detect if the function checkpoint_path is the folder/bucket path or the pt export filepath\n            ckpt_basepath = (\n                \"/\".join(checkpoint_path.split(\"/\")[:-1])\n                if checkpoint_path.endswith((\".pt\", \".pth\"))\n                else checkpoint_path\n            )\n            src_tokenizer = WordTokenizer.from_pretrained(ckpt_basepath + f\"/src_tokenizer_{tokenizers_type}.json\")\n            tgt_tokenizer = WordTokenizer.from_pretrained(ckpt_basepath + f\"/tgt_tokenizer_{tokenizers_type}.json\")\n            return src_tokenizer, tgt_tokenizer\n\n        checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=True)\n\n        Checkpoint.load_objects(to_load=to_load, checkpoint=checkpoint, strict=strict)\n        if logger is not None:\n            logger.info(\"Successfully resumed from a local checkpoint: %s\", checkpoint_path)\n\n    else:  # resume_method == \"bucket-s3\":\n        _, _, path = checkpoint_path.partition(\"s3://\")\n        bucket, _, key = path.partition(\"/\")\n\n        if bucket == \"\" or key == \"\":\n            raise InvalidCheckpointS3PathError()\n\n        src_tokenizer_key = key.split(\"/\")[0] + f\"/src_tokenizer_{tokenizers_type}.json\" if resume_tokenizers else None\n        tgt_tokenizer_key = key.split(\"/\")[0] + f\"/tgt_tokenizer_{tokenizers_type}.json\" if resume_tokenizers else None\n\n        s3 = S3Saver._make_s3_client()\n\n        if resume_tokenizers:  # Download tokenizers to temperary files\n            with (\n                tempfile.NamedTemporaryFile(delete=False) as src_tmp,\n                tempfile.NamedTemporaryFile(delete=False) as tgt_tmp,\n            ):\n                src_tmp_path = src_tmp.name\n                tgt_tmp_path = tgt_tmp.name\n            try:\n                try:\n                    s3.download_file(bucket, src_tokenizer_key, src_tmp_path)\n                    s3.download_file(bucket, tgt_tokenizer_key, tgt_tmp_path)\n                except Exception as e:\n                    raise S3FailedDownloadError(checkpoint_path, e) from e\n\n                src_tokenizer = WordTokenizer.from_pretrained(src_tmp_path)\n                tgt_tokenizer = WordTokenizer.from_pretrained(tgt_tmp_path)\n                logger.info(f\"Successfully resumed tokenizers from a bucket s3 checkpoint: {bucket}\")\n                return src_tokenizer, tgt_tokenizer\n            finally:\n                with suppress(OSError):\n                    os.remove(src_tmp_path)\n                    os.remove(tgt_tmp_path)\n\n        else:  # Download to_load to a temporary file\n            with tempfile.NamedTemporaryFile(delete=False) as tmp:\n                tmp_path = tmp.name\n            try:\n                try:\n                    s3.download_file(bucket, key, tmp_path)\n                except Exception as e:\n                    raise S3FailedDownloadError(checkpoint_path, e) from e\n\n                checkpoint = torch.load(tmp_path, map_location=device, weights_only=True)\n\n                # Fix model weights loading when the checkpoint layers' keys start with \"_orig_mod.\"\n                model_state_dict = checkpoint[\"model\"]\n                first_layer_key = next(iter(model_state_dict.keys()))\n                if first_layer_key.startswith(\"_orig_mod.\"):\n                    new_state_dict = {}\n                    prefix = \"_orig_mod.\"\n                    for k, v in model_state_dict.items():\n                        new_k = k[len(prefix) :] if k.startswith(prefix) else k\n                        new_state_dict[new_k] = v\n                    checkpoint[\"model\"] = new_state_dict\n\n                Checkpoint.load_objects(to_load=to_load, checkpoint=checkpoint, strict=strict)\n\n                if logger is not None:\n                    logger.info(f\"Successfully resumed training objects from a bucket s3 checkpoint: {checkpoint_path}\")\n            finally:\n                with suppress(OSError):\n                    os.remove(tmp_path)\n</code></pre>"},{"location":"modules/training_utils/#src.tfs_mt.training_utils.s3_upload","title":"<code>s3_upload(filepath, bucket, s3_key=None)</code>","text":"<p>Upload a file on S3 bucket.</p> Source code in <code>src/tfs_mt/training_utils.py</code> <pre><code>def s3_upload(filepath: str, bucket: str, s3_key: str | None = None) -&gt; None:\n    \"\"\"Upload a file on S3 bucket.\"\"\"\n\n    if not os.path.exists(filepath):\n        print(\"File was not found at the following path, skipping the upload\")\n        print(filepath)\n        return None\n\n    s3 = S3Saver._make_s3_client()\n    s3_key = s3_key or filepath.split(\"/\")[-1]  # Default key = filename\n\n    # Verify bucket existence\n    try:\n        s3.head_bucket(Bucket=bucket)\n    except botocore.exceptions.ClientError as exc:\n        print(f\"Bucket not found - {bucket}\")\n        if exc.response[\"Error\"][\"Code\"] == \"404\":\n            raise BucketNotFoundError(bucket) from exec\n        else:\n            raise\n\n    try:\n        s3.upload_file(filepath, bucket, s3_key)\n        print(f\"Uploaded '{filepath}' to s3://{bucket}/{s3_key}\")\n    except Exception as exc:\n        print(f\"Failed to upload '{filepath}' to S3: {exc}\")\n        raise\n</code></pre>"},{"location":"modules/training_utils/#src.tfs_mt.training_utils.save_config","title":"<code>save_config(config, output_dir, enable_ckpt=True)</code>","text":"<p>Save configuration to config-lock.yaml for result reproducibility.</p> Source code in <code>src/tfs_mt/training_utils.py</code> <pre><code>def save_config(\n    config: DictConfig | ListConfig,\n    output_dir: str,\n    enable_ckpt: bool = True,\n):\n    \"\"\"Save configuration to config-lock.yaml for result reproducibility.\"\"\"\n    with open(f\"{output_dir}/config-lock.yaml\", \"w+\") as f:\n        OmegaConf.save(config, f)\n    # Upload to S3 endpoint\n    if config.s3_bucket_name is not None and enable_ckpt:\n        s3_upload(\n            filepath=f\"{output_dir}/config-lock.yaml\",\n            bucket=config.s3_bucket_name,\n            s3_key=f\"{config.model_name}/config-lock.yaml\",\n        )\n</code></pre>"},{"location":"modules/training_utils/#src.tfs_mt.training_utils.setup_early_stopping","title":"<code>setup_early_stopping(trainer, evaluator, config)</code>","text":"<p>Setup early stopping.</p> Source code in <code>src/tfs_mt/training_utils.py</code> <pre><code>def setup_early_stopping(\n    trainer: Engine,\n    evaluator: Engine,\n    config: DictConfig | ListConfig,\n) -&gt; None:\n    \"\"\"Setup early stopping.\"\"\"\n\n    def score_fn(engine: Engine):\n        return engine.state.metrics[\"Bleu\"]\n\n    es = EarlyStopping(\n        patience=config.training_hp.early_stopping.patience,  # Considered in number of iterations\n        score_function=score_fn,\n        trainer=trainer,\n        min_delta=config.training_hp.early_stopping.min_delta,\n    )\n    evaluator.add_event_handler(Events.ITERATION_COMPLETED, es)\n</code></pre>"},{"location":"modules/training_utils/#src.tfs_mt.training_utils.setup_evaluator","title":"<code>setup_evaluator(config, model, metrics, device)</code>","text":"<p>Setup an evaluator with mixed precision training support.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>DictConfig | ListConfig</code> <p>Project config file.</p> required <code>model</code> <code>Module</code> <p>Transformer model.</p> required <code>metrics</code> <code>dict[str, Metric]</code> <p>Metrics to be used.</p> required <code>device</code> <code>device</code> <p>Device.</p> required <p>Returns:</p> Type Description <code>Engine | DeterministicEngine</code> <p>tuple[Engine | DeterministicEngine, Engine | DeterministicEngine]: Evaluator objects.</p> Source code in <code>src/tfs_mt/training_utils.py</code> <pre><code>def setup_evaluator(\n    config: DictConfig | ListConfig,\n    model: nn.Module,\n    metrics: dict[str, Metric],\n    device: torch.device,\n) -&gt; Engine | DeterministicEngine:\n    \"\"\"Setup an evaluator with mixed precision training support.\n\n    Args:\n        config (DictConfig | ListConfig): Project config file.\n        model (nn.Module): Transformer model.\n        metrics (dict[str, Metric]): Metrics to be used.\n        device (torch.device): Device.\n\n    Returns:\n        tuple[Engine | DeterministicEngine, Engine | DeterministicEngine]: Evaluator objects.\n    \"\"\"\n\n    # Gradient scaler is not required during evaluation.\n    # https://docs.pytorch.org/tutorials/recipes/recipes/amp_recipe.html#inference-evaluation\n\n    amp_dtype_dict = {\"float32\": torch.float32, \"bfloat16\": torch.bfloat16, \"float16\": torch.float16}\n\n    @torch.no_grad()\n    def eval_one_iter(engine: Engine, batch: dict[str, torch.Tensor | str]) -&gt; tuple[torch.Tensor, torch.Tensor]:\n        # NOTE See train_one_iter function for code explanation\n\n        src_sequence = batch[\"src\"].to(device, non_blocking=True, dtype=torch.long)\n        tgt_sequence = batch[\"tgt\"].to(device, non_blocking=True, dtype=torch.long)\n        src_mask = batch[\"src_mask\"].to(device, non_blocking=True, dtype=torch.long)\n        tgt_mask = batch[\"tgt_mask\"].to(device, non_blocking=True, dtype=torch.long)\n\n        tgt_output_label = tgt_sequence[:, 1:]\n\n        tgt_input_sequence = tgt_sequence[:, :-1]\n\n        model.eval()\n\n        with autocast(\n            device.type, dtype=amp_dtype_dict[config.training_hp.amp_dtype], enabled=config.training_hp.use_amp\n        ):\n            output_logits = model(src_sequence, tgt_input_sequence, src_mask, tgt_mask)\n\n        return output_logits, tgt_output_label\n\n    test_evaluator = Engine(eval_one_iter)\n\n    for name, metric in metrics.items():\n        metric.attach(test_evaluator, name)\n\n    return test_evaluator\n</code></pre>"},{"location":"modules/training_utils/#src.tfs_mt.training_utils.setup_exp_logging","title":"<code>setup_exp_logging(config, trainer, optimizer, evaluator, metrics, model=None, return_all_loggers=False)</code>","text":"<p>Setup Experiment Tracking with WandB and Trackio loggers.</p> <p>Using common.setup_wandb_logging which setup an ignite's Engine compatible WandB logger. It takes as kwargs wandb.init compatible arguments. Same for trackio.</p> <p>References: 1. setup_wandb_logging documentation [link] 2. WandBLogger documentation [link] 3. wandb.init documentation [link] 4. trackio.init documentation [link]</p> Source code in <code>src/tfs_mt/training_utils.py</code> <pre><code>def setup_exp_logging(\n    config: DictConfig | ListConfig,\n    trainer: Engine,\n    optimizer: Optimizer | dict[str, Optimizer],\n    evaluator: Engine | dict[str, Engine],\n    metrics: dict[str, Metric],\n    model: nn.Module | None = None,\n    return_all_loggers: bool = False,\n) -&gt; WandBLogger | tuple[WandBLogger, TrackioLogger]:\n    \"\"\"Setup Experiment Tracking with WandB and Trackio loggers.\n\n    Using common.setup_wandb_logging which setup an ignite's Engine compatible WandB logger.\n    It takes as kwargs wandb.init compatible arguments.\n    Same for trackio.\n\n    References:\n    1. setup_wandb_logging documentation [[link](https://docs.pytorch.org/ignite/contrib/engines.html#ignite.contrib.engines.common.setup_wandb_logging)]\n    2. WandBLogger documentation [[link](https://docs.pytorch.org/ignite/generated/ignite.handlers.wandb_logger.html#ignite.handlers.wandb_logger.WandBLogger)]\n    3. wandb.init documentation [[link](https://docs.wandb.ai/models/ref/python/functions/init)]\n    4. trackio.init documentation [[link](https://huggingface.co/docs/trackio/en/api#trackio.init)]\n    \"\"\"\n\n    wandb_logger = WandBLogger(\n        entity=config.wandb_organization,\n        project=config.model_base_name,\n        name=config.model_name,\n        config=config._content,\n        tags=[\"pytorch\", \"nlp\", \"machine-translation\"],\n    )\n\n    wandb_logger.attach_output_handler(\n        trainer,\n        event_name=Events.ITERATION_COMPLETED(every=config.log_every_iters),\n        tag=\"training\",\n        # metric_names=list(trainer.state.metrics.keys()),  # Data recorded during trainig: loss, number of seen tokens\n        output_transform=lambda loss: loss,\n        # output_transform=lambda loss: {\"Loss\": loss}\n    )\n\n    wandb_logger.attach_output_handler(\n        evaluator,\n        event_name=Events.EPOCH_COMPLETED(every=1),\n        tag=\"test_eval\",\n        metric_names=list(metrics.keys()),\n        global_step_transform=lambda *_: trainer.state.iteration,\n    )\n\n    wandb_logger.attach_opt_params_handler(\n        trainer, event_name=Events.ITERATION_STARTED(every=config.log_every_iters), optimizer=optimizer, param_name=\"lr\"\n    )\n\n    if model is not None:  # Watch model to log gradients\n        wandb_logger.watch(model)\n\n    if not return_all_loggers:\n        return wandb_logger\n\n    # Custom trackio directory https://huggingface.co/docs/trackio/en/environment_variables#trackiodir\n    os.environ[\"TRACKIO_DIR\"] = config.output_dir + \"/trackio\"\n\n    trackio_logger = setup_trackio_logging(\n        trainer,\n        optimizer,\n        evaluator,\n        config.log_every_iters,\n        # trackio.init kwargs\n        project=config.model_base_name,\n        name=config.model_name,\n        config=config._content,\n    )\n    return wandb_logger, trackio_logger\n</code></pre>"},{"location":"modules/training_utils/#src.tfs_mt.training_utils.setup_handlers","title":"<code>setup_handlers(trainer, evaluator, config, to_save_train=None, to_save_test=None, enable_ckpt=True)</code>","text":"<p>Setup Ignite handlers.</p> Source code in <code>src/tfs_mt/training_utils.py</code> <pre><code>def setup_handlers(\n    trainer: Engine,\n    evaluator: Engine,\n    config: DictConfig | ListConfig,\n    to_save_train: dict | None = None,\n    to_save_test: dict | None = None,\n    enable_ckpt: bool = True,\n) -&gt; None:\n    \"\"\"Setup Ignite handlers.\"\"\"\n\n    if enable_ckpt == 0:  # Setup checkpointing\n        # Setup checkpoints savers\n        disk_saver = DiskSaver(os.path.join(config.output_dir, \"checkpoints\"), require_empty=False)\n        s3_saver = (\n            S3Saver(bucket=config.s3_bucket_name, prefix=config.model_name + \"/\")\n            if config.s3_bucket_name is not None\n            else None\n        )\n\n        # Training checkpointing.\n        # Do it locally only if s3 checkpointing is disabled to save disk space in cloud instance.\n        ckpt_handler_train = Checkpoint(\n            to_save_train,\n            save_handler=s3_saver if config.s3_bucket_name is not None else disk_saver,\n            filename_prefix=config.model_base_name,\n            n_saved=config.checkpoints_retain_n,\n        )\n        train_ckpt_events = (\n            Events.ITERATION_COMPLETED(every=config.save_every_iters)\n            | Events.EPOCH_COMPLETED(every=1)\n            | Events.TERMINATE\n            | Events.COMPLETED\n        )\n        trainer.add_event_handler(train_ckpt_events, ckpt_handler_train)\n\n        # Evaluation checkpointing.\n        # Do it locally only if s3 checkpointing is disabled to save disk space in cloud instance.\n        global_step_transform = None\n        if to_save_train.get(\"trainer\", None) is not None:\n            global_step_transform = global_step_from_engine(to_save_train[\"trainer\"])\n        ckpt_handler_test = Checkpoint(\n            to_save_test,\n            save_handler=s3_saver if config.s3_bucket_name is not None else disk_saver,\n            filename_prefix=\"best\",\n            n_saved=config.checkpoints_retain_n,\n            global_step_transform=global_step_transform,\n            score_name=\"test_bleu\",\n            score_function=Checkpoint.get_default_score_fn(\"Bleu\"),\n        )\n        test_ckpt_events = Events.EPOCH_COMPLETED(every=1) | Events.TERMINATE | Events.COMPLETED\n        evaluator.add_event_handler(test_ckpt_events, ckpt_handler_test)\n\n    # Time limit reached policy to stop training. Mainly used in Kaggle due to 12 hours run limit.\n    if config.time_limit_sec != -1:\n        print(f\"Setting up training time limit to {int(config.time_limit_sec) / 3600} hours.\")\n        trainer.add_event_handler(Events.ITERATION_COMPLETED, TimeLimit(config.time_limit_sec))\n\n    # Iterations and epochs progress bars\n    ProgressBar(persist=True, bar_format=\"\").attach(\n        trainer, event_name=Events.EPOCH_STARTED, closing_event_name=Events.COMPLETED\n    )\n    ProgressBar(persist=False).attach(\n        trainer, metric_names=\"all\", event_name=Events.ITERATION_COMPLETED(every=config.update_pbar_every_iters)\n    )\n\n    if torch.cuda.is_available():\n        GpuInfo().attach(trainer, name=\"gpu\")\n</code></pre>"},{"location":"modules/training_utils/#src.tfs_mt.training_utils.setup_lr_lambda_fn","title":"<code>setup_lr_lambda_fn(config)</code>","text":"<p>Setup function that will govern the learning rate scheduling.</p> <p>It supports two scheduling strategies: Warmup-Stable-Decay (WSD) and the original lr scheduling from Attention Is All You Need.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>DictConfig | ListConfig</code> <p>Project config file.</p> required <p>Returns:</p> Type Description <code>Callable[[int], float]</code> <p>Callable[[int], float]: Lambda function mapping step number to LR multiplier.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>Raised when config.training_hp.lr_scheduler.type is not \"wsd\" nor \"original\".</p> Source code in <code>src/tfs_mt/training_utils.py</code> <pre><code>def setup_lr_lambda_fn(config: DictConfig | ListConfig) -&gt; Callable[[int], float]:\n    \"\"\"Setup function that will govern the learning rate scheduling.\n\n    It supports two scheduling strategies: Warmup-Stable-Decay (WSD) and the original lr scheduling from Attention Is All You Need.\n\n    Args:\n        config (DictConfig | ListConfig): Project config file.\n\n    Returns:\n        Callable[[int], float]: Lambda function mapping step number to LR multiplier.\n\n    Raises:\n        ValueError: Raised when config.training_hp.lr_scheduler.type is not \"wsd\" nor \"original\".\n    \"\"\"\n\n    min_lr = config.training_hp.lr_scheduler.min_lr\n    max_lr = config.training_hp.lr_scheduler.max_lr\n\n    total_iters = config.num_train_iters_per_epoch * config.training_hp.num_epochs\n    warmup_iters = config.training_hp.lr_scheduler.warmup_iters\n    stable_iters = config.training_hp.lr_scheduler.stable_iters_prop * (total_iters - warmup_iters)\n    decay_iters = total_iters - warmup_iters - stable_iters\n\n    d_model = config.model_configs[config.chosen_model_size].d_model\n\n    def wsd_lr_lambda(step):\n        if step &lt; warmup_iters:  # Warmup phase\n            return max_lr * ((step + 1) / warmup_iters)\n\n        elif step &lt; stable_iters + warmup_iters:  # Stable phase\n            return max_lr\n\n        else:  # Decay phase\n            step = step - warmup_iters - stable_iters\n            cosine_decay = 0.5 * (1 + math.cos(math.pi * step / decay_iters))\n            return min_lr + (max_lr - min_lr) * cosine_decay\n\n    def original_lr_lambda(step):\n        step = max(step, 1)  # Avoids zero division in step**-0.5\n        return (d_model**-0.5) * min(step**-0.5, step * (warmup_iters**-1.5))\n\n    if config.training_hp.lr_scheduler.type == \"wsd\":\n        return wsd_lr_lambda\n    elif config.training_hp.lr_scheduler.type == \"original\":\n        return original_lr_lambda\n    else:\n        raise ValueError(f\"Invalid learning rate scheduler type, got {config.training_hp.lr_scheduler.type}\")\n</code></pre>"},{"location":"modules/training_utils/#src.tfs_mt.training_utils.setup_output_dir","title":"<code>setup_output_dir(config)</code>","text":"<p>Create output folder.</p> Source code in <code>src/tfs_mt/training_utils.py</code> <pre><code>def setup_output_dir(config: DictConfig | ListConfig) -&gt; str:\n    \"\"\"Create output folder.\"\"\"\n    output_dir = config.output_dir\n\n    now = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n    name = f\"{now}-min_lr-{config.training_hp.lr_scheduler.min_lr}-max_lr-{config.training_hp.lr_scheduler.max_lr}\"\n    output_dir = os.path.join(config.output_dir, name)\n    os.makedirs(output_dir, exist_ok=True)\n\n    return output_dir\n</code></pre>"},{"location":"modules/training_utils/#src.tfs_mt.training_utils.setup_trainer","title":"<code>setup_trainer(config, model, optimizer, lr_scheduler, loss_fn, device)</code>","text":"<p>Setup a trainer with mixed precision training support.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>DictConfig | ListConfig</code> <p>Project config file.</p> required <code>model</code> <code>Module</code> <p>Transformer model.</p> required <code>optimizer</code> <code>Optimizer</code> <p>Optimizer.</p> required <code>loss_fn</code> <code>Module</code> <p>Loss function.</p> required <code>device</code> <code>device</code> <p>Device.</p> required <p>Returns:</p> Type Description <code>Engine | DeterministicEngine</code> <p>Engine | DeterministicEngine: Trainer object.</p> Source code in <code>src/tfs_mt/training_utils.py</code> <pre><code>def setup_trainer(\n    config: DictConfig | ListConfig,\n    model: nn.Module,\n    optimizer: Optimizer,\n    lr_scheduler: LRScheduler,\n    loss_fn: nn.Module,\n    device: torch.device,\n) -&gt; Engine | DeterministicEngine:\n    \"\"\"Setup a trainer with mixed precision training support.\n\n    Args:\n        config (DictConfig | ListConfig): Project config file.\n        model (nn.Module): Transformer model.\n        optimizer (Optimizer): Optimizer.\n        loss_fn (nn.Module): Loss function.\n        device (torch.device): Device.\n\n    Returns:\n        Engine | DeterministicEngine: Trainer object.\n    \"\"\"\n\n    # Gradient scaler for mixed precision training. Not required for bfloat16 training, cause it has the same range of float32.\n    # It helps prevent gradients with small magnitudes from underflowing when training with mixed precision.\n    scaler = GradScaler(device, enabled=config.training_hp.use_amp)\n\n    amp_dtype_dict = {\"float32\": torch.float32, \"bfloat16\": torch.bfloat16, \"float16\": torch.float16}\n\n    def train_one_iter(engine: Engine | DeterministicEngine, batch: dict[str, torch.Tensor | str]) -&gt; dict[str, Any]:\n        # non_blocking asynchronously transfers tensor from CPU to device. More here: https://docs.pytorch.org/tutorials/intermediate/pinmem_nonblock.html\n        src_sequence = batch[\"src\"].to(device, non_blocking=True, dtype=torch.long)\n        tgt_sequence = batch[\"tgt\"].to(device, non_blocking=True, dtype=torch.long)\n        src_mask = batch[\"src_mask\"].to(device, non_blocking=True, dtype=torch.long)\n        # Mask is not shrinked accordingly to tgt_sequence here. It will be handled during attention processing.\n        tgt_mask = batch[\"tgt_mask\"].to(device, non_blocking=True, dtype=torch.long)  # [:, :-1]\n\n        # Shifted target sequence as label for teacher forcing. Reshape to 1D tensor to later compute loss\n        tgt_output_label = tgt_sequence[:, 1:]\n\n        tgt_input_sequence = tgt_sequence[:, :-1]\n\n        # Count how many tokens encoder and decoder see during training, excluding SOS and EOS tokens\n        num_src_tokens = src_mask.to(torch.int8).sum().item() - 2 * src_mask.size(0)\n        num_tgt_tokens = tgt_mask.to(torch.int8).sum().item() - 2 * tgt_mask.size(0)\n        engine.state.tokens_seen_src = getattr(engine.state, \"tokens_seen_src\", 0) + num_src_tokens\n        engine.state.tokens_seen_tgt = getattr(engine.state, \"tokens_seen_tgt\", 0) + num_tgt_tokens\n\n        model.train()\n\n        optimizer.zero_grad()\n\n        # Mixed precision training if enabled in config. Reference: https://docs.pytorch.org/tutorials/recipes/recipes/amp_recipe.html\n        # autocast will automatically manage which operations to run in FP16 and which ones to run in FP32.\n        # eg. matmul will cast to FP16 and it's a crucial part of the whole pipeline.\n        # Here the complete list of FP16 supported modules: https://docs.pytorch.org/docs/stable/amp.html#cuda-ops-that-can-autocast-to-float16\n        # NOTE Switching to torch.bfloat16 for better accuracy and same efficiency of float16, more on this here: https://www.cerebras.ai/blog/to-bfloat-or-not-to-bfloat-that-is-the-question\n        with autocast(\n            device.type, dtype=amp_dtype_dict[config.training_hp.amp_dtype], enabled=config.training_hp.use_amp\n        ):\n            output_logits = model(src_sequence, tgt_input_sequence, src_mask, tgt_mask)\n\n            if config.training_hp.loss.type == \"crossentropy\":\n                # pred shape: [B*S, V]  (B: batch size, S: sequence length, V: vocabulary size)\n                # target shape: [B*S]\n                pred = output_logits.reshape(-1, output_logits.size(-1))\n                target = tgt_output_label.reshape(-1)\n\n                loss = loss_fn(pred, target)\n                loss /= num_tgt_tokens  # Loss rescaling based on processed target tokens\n\n            elif config.training_hp.loss.type == \"KLdiv-labelsmoothing\":\n                output_log_proba = F.log_softmax(output_logits, dim=-1)\n\n                pred = output_log_proba.reshape(-1, output_log_proba.size(-1))\n                target = tgt_output_label.reshape(-1)\n\n                loss = loss_fn(pred, target)\n                loss /= num_tgt_tokens\n\n            else:\n                raise ValueError(\"Loss type not supported\")\n\n        scaler.scale(loss).backward()\n\n        # Gradient clipping to stabilize training avoiding exploding gradients\n        grad_norm_before_clipping = compute_grad_norm(model.parameters())\n        if config.training_hp.max_gradient_norm &gt; 0:\n            nn.utils.clip_grad_norm_(model.parameters(), config.training_hp.max_gradient_norm)\n\n        scaler.step(optimizer)\n        scaler.update()\n        lr_scheduler.step()\n\n        metric = {\n            \"train_loss\": loss.item(),\n            \"tokens_seen_src_cum\": getattr(engine.state, \"tokens_seen_src\", 0),\n            \"tokens_seen_tgt_cum\": getattr(engine.state, \"tokens_seen_tgt\", 0),\n            \"tokens_seen_tot_cum\": getattr(engine.state, \"tokens_seen_src\", 0)\n            + getattr(engine.state, \"tokens_seen_tgt\", 0),\n            \"grad_norm_before_clipping\": grad_norm_before_clipping,\n        }\n        engine.state.metrics = metric\n\n        return metric\n\n    trainer = Engine(train_one_iter)\n\n    # Initialize token seen during training counters\n    @trainer.on(Events.EPOCH_STARTED)\n    def init_token_seen():\n        trainer.state.tokens_seen_src = 0\n        trainer.state.tokens_seen_tgt = 0\n\n    return trainer\n</code></pre>"}]}